<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-08-31T03:35:16+00:00</updated><id>/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</title><link href="/blog/2025/paretoq-summary/" rel="alternate" type="text/html" title="ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization" /><published>2025-03-23T00:00:00+00:00</published><updated>2025-03-23T00:00:00+00:00</updated><id>/blog/2025/paretoq-summary</id><content type="html" xml:base="/blog/2025/paretoq-summary/"><![CDATA[<style>  
li {  
    font-size: 1.1rem; /* Adjust as needed */  
}  
</style>

<h1 id="paretoq-scaling-laws-in-extremely-low-bit-llm-quantization"><a href="https://arxiv.org/abs/2502.02631">ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</a></h1>
<blockquote>
  <p>[TL;DR]<br />
The paper introduces ParetoQ, a unified framework that compares LLM quantization across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit settings. It discovers a key transition between 2-bit and 3-bit quantization, where models retain their original representations at 3-bit and higher, but undergo substantial changes at lower bit widths. ParetoQ shows that 2-bit quantization is a strong alternative to 4-bit due to its superior efficiency-accuracy trade-offs.</p>
</blockquote>

<h2 id="highlights">Highlights</h2>
<ul>
  <li>Demonstrates that 2-bit, 3-bit, and ternary quantization often outperform 4-bit in terms of accuracy-memory trade-offs.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/pareto-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/pareto-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/pareto-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/pareto.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>
<ul>
  <li>Identifies a sharp transition between 2-bit and 3-bit quantization, where 3-bit models and above retain pre-trained distributions, while 2-bit models undergo major representation shifts.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/compansate_reconstruct-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/compansate_reconstruct-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/compansate_reconstruct-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/compansate_reconstruct.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>
<ul>
  <li>Quantization-aware training (QAT) consistently surpasses both post-training quantization (PTQ, no fine-tuning) and QAT from scratch.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/qat_vs_ptq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/qat_vs_ptq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/qat_vs_ptq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/qat_vs_ptq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>
<ul>
  <li>Propose a refined quantization functions, Stretched Elastic Quant (SEQ), for low-bit settings.
\(\mathbf{W}_Q^i = \alpha \left( \left\lfloor \text{Clip} \left( \frac{\mathbf{W}_R^i}{\alpha}, -1, 1 \right) \times \frac{k}{2} - 0.5 \right\rfloor + 0.5 \right) / k \times 2\)
\(\mathbf{W}_Q^i = \alpha \mathbf{\hat{W}}_Q^i
= 
\begin{cases} 
\alpha \cdot \text{Sign}(\mathbf{W}_R^i), &amp; \text{if } N_{bit} = 1 \\ 
\alpha \left( \left\lfloor \text{Clip} \left( \frac{\mathbf{W}_R^i}{\alpha}, -1, 1 \right) \times \frac{k}{2} - 0.5 \right\rfloor + 0.5 \right) / k \times 2, &amp; \text{if } N_{bit} = 1.58, 2 \\ 
\alpha \lfloor \text{Clip} \left( \frac{\mathbf{W}_R^i}{\alpha}, n, p \right) \rfloor, &amp; \text{if } N_{bit} = 3, 4 
\end{cases}\)</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-10 mt-3 mt-md-0 offset-1">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/seq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/seq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/seq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/seq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h2 id="summary">Summary</h2>
<ul>
  <li><strong>Observation 1</strong>: Recent studies on scaling laws in the low-precision domain have reached conflicting conclusions.
    <ul>
      <li><a href="https://proceedings.mlr.press/v202/dettmers23a">Dettmers &amp; Zettlemoyer</a> and <a href="https://arxiv.org/abs/2411.04330">Kumar et al</a> argue that 4-bit or 6-bit quantization often resides on the Pareto frontier, balancing accuracy and efficiency.</li>
      <li>In contrast, <a href="https://storage.prod.researchhub.com/uploads/papers/2024/02/29/2402.17764.pdf">Ma et al.</a> and <a href="https://arxiv.org/abs/2407.12327">Kaushal et al.</a> suggest that bit-widths as low as 1.58 bits per parameter offer significant potential for optimal scaling trade-offs.</li>
    </ul>
  </li>
  <li><strong>Observation 2</strong>: Prior studies overlook the impact of the training scheme, denoted as \(\mathbf{S}_{\text{train}}\), and the bit-specific quantization function \(\mathcal{F}\).</li>
  <li><strong>The problem statement</strong>: How to determine the optimal trade-off between bit-width and model size while ensuring accuracy?</li>
  <li><strong>The solution</strong>: The authors propose a scaling law \(\mathcal{L}(\mathcal{N}, \mathcal{D}, \mathcal{P}, \mathbf{S}_{\text{train}}, \mathcal{F})\) comprising five dimensions, and systematically optimizes quantization functions and training schemes across different bit-widths.
    <ul>
      <li>Introduces Stretched Elastic Quantization (SEQ), which balances quantization grids for 2-bit and ternary settings.</li>
      <li>Applies learnable quantization ranges, outperforming static min-max methods.</li>
    </ul>
  </li>
  <li><strong>The proposed framework</strong>: The quantized framework evaluates models under 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit precision.</li>
</ul>

<h2 id="experiments">Experiments</h2>

<ul>
  <li>Accuracy-compression and Accuracy-speed Trade-off</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-12 mt-3 mt-md-0 offset-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/pareto_models_latency-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/pareto_models_latency-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/pareto_models_latency-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/pareto_models_latency.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<ul>
  <li>2-bit / 3-bit / 4-bit Comparisons</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-12 mt-3 mt-md-0 offset-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/bits_2_3_4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/bits_2_3_4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/bits_2_3_4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/bits_2_3_4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<ul>
  <li>1.58-bit Comparison on Sub-8B Models
    <ul>
      <li>Note: floating-point LLaMA-3 3B model achieves 69.9 accuracy</li>
    </ul>
  </li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/sub_8b_model-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/sub_8b_model-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/sub_8b_model-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/sub_8b_model.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<ul>
  <li>Main Results</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-8 mt-3 mt-md-0 offset-2">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/paretoq/main_table-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/paretoq/main_table-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/paretoq/main_table-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/paretoq/main_table.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h2 id="conclusions">Conclusions</h2>
<ul>
  <li>2-bit quantization outperforms 4-bit in efficiency-accuracy trade-offs.</li>
  <li>Fine-tuning is crucial for sub-4-bit quantization, especially for binary and ternary models.</li>
  <li>Quantization-aware training (QAT) finetuning consistently surpasses both post-training quantization (PTQ, no fine-tuning) and QAT from scratch</li>
  <li>QAT serves as a compensation mechanism for bit widths above 2-bit and as a reconstruction process for bit widths below 2-bit, where weights adapt to form new representations.</li>
  <li>Extreme low-bit quantization is highly sensitive to quantization function selection, with no single optimal function for all bit widths.</li>
</ul>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="llm" /><category term="quantization" /><summary type="html"><![CDATA[ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization]]></summary></entry><entry><title type="html">Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models</title><link href="/blog/2025/minions-summary/" rel="alternate" type="text/html" title="Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models" /><published>2025-03-16T00:00:00+00:00</published><updated>2025-03-16T00:00:00+00:00</updated><id>/blog/2025/minions-summary</id><content type="html" xml:base="/blog/2025/minions-summary/"><![CDATA[<style>  
li {  
    font-size: 1.1rem; /* Adjust as needed */  
}  
</style>

<h1 id="minions-cost-efficient-collaboration-between-on-device-and-cloud-language-models"><a href="https://arxiv.org/abs/2502.15964">Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models</a></h1>
<blockquote>
  <p>[TL;DR]<br />
MinionS is a collaboration protocol between <strong>local small LMs</strong> and <strong>remote frontier LMs</strong>, significantly reducing cloud inference costs while maintaining near-frontier accuracy by decomposing complex tasks into simpler subtasks executed locally in parallel.</p>
</blockquote>

<h2 id="highlights">Highlights</h2>
<ul>
  <li>Proposes a collaboration protocol between <strong>local small LMs</strong> and <strong>remote frontier LMs</strong></li>
  <li>Proposes two protocols: Minion (naïve chat-based) and MinionS (task decomposition-based)</li>
  <li>MinionS reduces cloud inference costs by <strong>5.7×</strong> on average and recovers <strong>97.9%</strong> of frontier LM performance</li>
  <li>Conducts detailed analyses on model choice, parallel workload scaling, and sequential communication strategies</li>
</ul>

<h2 id="summary">Summary</h2>
<ul>
  <li><strong>Observation 1</strong>: Large model can perform data-intensive reasoning, but accessing these models is expensive.</li>
  <li><strong>Observation 2</strong>: Small local models run on-device with nocost, but they struggle with multi-step instructions and long context reasoning.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/slm-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/slm-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/slm-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/slm.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Small language models" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>
<ul>
  <li><strong>The problem statement</strong>: Reducing the cost of cloud-based inference while maintaining performance by enabling effective collaboration between small, on-device LMs and large, remote LMs.</li>
  <li><strong>The solution</strong>: MinionS leverages the remote LM to decompose complex queries into simpler subtasks which are executed in parallel by local LMs on smaller document chunks, improving accuracy and reducing remote inference costs.</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Minions Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<ul>
  <li><strong>Finding 1</strong>: Practical Effectiveness: Achieves near-equivalent accuracy to large remote-only models at just a fraction (around 18%) of the cost.</li>
  <li><strong>Finding 2</strong>: Effective collaboration is achievable starting from a 3B-parameter local model, with larger local models (8B) further improving accuracy and cost efficiency.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-10 mt-3 mt-md-0 offset-1">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/slm_size-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/slm_size-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/slm_size-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/slm_size.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Small language models size" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h2 id="experiments">Experiments</h2>

<h4 id="comparison-with-baselines">Comparison with Baselines</h4>
<ul>
  <li>Minion (naïve protocol) achieves 30.4× cost reduction but with 87% of the accuracy of the remote-only model.</li>
  <li>MinionS substantially improves on Minion by achieving 97.9% of the accuracy at only 18% of the cost compared to remote-only inference.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-4 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/baselines-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/baselines-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/baselines-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/baselines.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Minions baselines" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
 
    </div>
    <div class="col-sm-8 mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/table-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/table-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/table-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/table.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="analysis-of-parallel-workloads">Analysis of Parallel Workloads</h4>
<ul>
  <li>Three parameters configured by RemoteLM for increasing the degree of task decomposition:
    <ul>
      <li>(1) <strong>Number of tasks per round</strong>: How many simpler jobs or subtasks the remote model creates for the local models, enabling parallel execution locally. (i.e. “Extract the ARR for Q1 of 2014”)</li>
      <li>(2) <strong>Number of samples per task</strong>: Number of repeated attempts (samples) made by the local language model (LocalLM) for each individual subtask (i.e. number of generations created with LocalLM, ≥ 1).</li>
      <li>(3) <strong>Chunk size</strong>: chunk by page, chunk by paragraph, etc; smaller chunks will send more information to cloud.</li>
    </ul>
  </li>
  <li>Increasing the number of tasks, samples per task, and chunking granularity improves accuracy but increases cost. Task decomposition and chunk size offer a more cost-effective trade-off.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm-10 mt-3 mt-md-0 offset-1">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/scaling-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/scaling-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/scaling-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/scaling.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="sequential-communication">Sequential Communication</h4>
<ul>
  <li>Increasing sequential communication rounds can improve accuracy, but also increases cost.</li>
  <li>Strategies like using a scratchpad for intermediate steps slightly improve the cost-accuracy tradeoff.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-6 mt-3 mt-md-0 offset-3">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/sequential-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/sequential-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/sequential-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/sequential.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="retrieval-augmented-generation-rag-comparison">Retrieval-Augmented Generation (RAG) Comparison</h4>
<ul>
  <li>RAG excels in structured extraction tasks but struggles with tasks requiring synthesis from dispersed information, where MinionS provides superior token efficiency and narrative coherence.</li>
  <li>From Figure 8 left, none of the RAG configurations are are able to match the quality of Minion at the same low cost.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm-10 mt-3 mt-md-0 offset-1">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/minions/rag-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/minions/rag-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/minions/rag-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/minions/rag.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h2 id="conclusions">Conclusions</h2>

<ul>
  <li>MinionS efficiently distributes workload between local and remote LMs, significantly reducing cloud inference costs while preserving accuracy.</li>
  <li>This collaboration approach becomes increasingly effective with advancements in local LM capabilities, showing strong potential for future cost-efficient systems.</li>
</ul>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="llm" /><category term="federated" /><category term="distributed" /><summary type="html"><![CDATA[Minions: Cost-efficient Collaboration Between On-device and Cloud Language Models]]></summary></entry><entry><title type="html">Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</title><link href="/blog/2025/PEQA-summary/" rel="alternate" type="text/html" title="Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization" /><published>2025-02-23T00:00:00+00:00</published><updated>2025-02-23T00:00:00+00:00</updated><id>/blog/2025/PEQA-summary</id><content type="html" xml:base="/blog/2025/PEQA-summary/"><![CDATA[<style>  
li {  
    font-size: 1.1rem; /* Adjust as needed */  
}  
</style>

<h1 id="memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization"><a href="https://arxiv.org/abs/2305.14152">Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</a></h1>
<blockquote>
  <p>[TL;DR]<br />
PEQA is a novel fine-tuning approach that integrates Parameter-Efficient Fine-Tuning (PEFT) with <em>weight-only</em> quantized LLMs by updating only the quantization scales, preserving low-bit integer weight matrices. This results in huge memory savings, seamless task adaptation, and inference acceleration.</p>
</blockquote>

<h2 id="highlights">Highlights</h2>
<ul>
  <li>Integrates PEFT with quantized LLMs, updating only the <strong>quantization scales</strong> while keeping integer matrices frozen.</li>
  <li><strong>Reduces memory</strong> consumption during <strong>fine-tuning</strong> and <strong>deployment</strong>, making LLM adaptation feasible even for resource-constrained settings.</li>
  <li>Maintains quantization benefits post fine-tuning, ensuring <strong>accelerated inference</strong>.</li>
  <li>Demonstrates resilience in performance recovery, even for <strong>sub-4-bit quantized models</strong>, on large-scale instruction datasets.</li>
  <li><strong>Scales up to 65B parameter models</strong> while achieving performance close to full-precision LoRA fine-tuning.</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/results_overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/results_overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/results_overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/peqa/results_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="PEQA Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h2 id="summary">Summary</h2>
<ul>
  <li><strong>Problem Statement</strong>: LLM fine-tuning is memory-intensive, even with PEFT such as LoRA, as full-precision weights remain a bottleneck. Quantization can reduce memory but is typically applied post-training, which limits the adaptability.</li>
  <li><strong>Solution</strong>: PEQA bridges this gap by fine-tuning only the <em>quantization scales</em> of a pre-quantized LLM while keeping the integer weights frozen. This enables task-specific adaptation with minimal overhead.</li>
  <li><strong>PEQA Framework</strong>:
    <ul>
      <li><strong>Step 1 Decomposition</strong>: Pre-trained model weights are quantized into sub-4-bit integers with associated scaling factors.</li>
      <li><strong>Step 2 Fine-tuning</strong>: Only the quantization scales are updated while maintaining the frozen integer matrix, significantly reducing learnable parameters.</li>
    </ul>
  </li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/framework_overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/framework_overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/framework_overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/peqa/framework_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="PEQA Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h2 id="key-advantages">Key Advantages</h2>

<h4 id="memory-efficiency">Memory Efficiency</h4>
<ul>
  <li><strong>Fine-tunes only the quantization scales</strong>, significantly reducing memory overhead.</li>
  <li><strong>Optimized for low-bit integer quantization (≤ 4-bit)</strong> while maintaining high accuracy.</li>
</ul>

<h4 id="seamless-task-switching">Seamless Task Switching</h4>
<ul>
  <li>PEQA enables <strong>quick and efficient adaptation</strong> across different tasks by <em>swapping</em> quantization scales instead of retraining entire models.</li>
</ul>

<h4 id="faster-inference">Faster Inference</h4>
<ul>
  <li>The <strong>frozen integer matrix remains intact</strong>, ensuring post-fine-tuning speedup using quantized inference kernels.<br />
<br /></li>
</ul>

<h2 id="experiments">Experiments</h2>

<h4 id="memory-and-general-comparison">Memory and General Comparison</h4>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/framework_comparison-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/framework_comparison-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/framework_comparison-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/peqa/framework_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h4 id="peqa-vs-qat-vs-peftptq">PEQA vs. QAT vs. PEFT+PTQ</h4>
<ul>
  <li>PEQA achieves performance close to QAT, significantly outperforming LoRA + PTQ at <strong>3-bit and 4-bit</strong> precision.</li>
  <li>Lower perplexity indicates effective fine-tuning of quantized models without sacrificing accuracy.</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/peqa/peqa_vs_baselines_ppl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h4 id="instruction-tuning-with-alpaca-dataset">Instruction-Tuning with Alpaca Dataset</h4>
<ul>
  <li>Evaluated on <strong>common-sense reasoning and in-context learning tasks</strong> (ARC, PIQA, HellaSwag).</li>
  <li><strong>Performance comparable to LoRA</strong>, with additional memory savings and inference acceleration.</li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/peqa/peqa_instruction_tuning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h2 id="notations">Notations</h2>

<h4 id="quantized-weights-and-fine-tuning">Quantized Weights and Fine-Tuning</h4>
<ul>
  <li>Weight-only asymmetric quantization:<br />
Given a fully-connected layer \(\mathbf{W}_0 \in \mathbb{R}^{n \times m}\), a given bit-width \(b\), per-channel scales and zero-points \(\mathbf{s}_0, \mathbf{z}_0 \in \mathbb{R}^{n \times 1}\), asymmetric quantized pre-trained weights \(\widehat{\mathbf{W}}_0\) can be written as</li>
</ul>

\[\widehat{\mathbf{W}}_0 = \mathbf{s}_0 \cdot \overline{\mathbf{W}}_0 = \mathbf{s}_0 \cdot \left( \text{clamp} \left( \left\lfloor \frac{\mathbf{W}_0}{\mathbf{s}_0} \right\rfloor + \mathbf{z}_0, 0, 2^b - 1 \right) - \mathbf{z}_0 \right),\]

<ul>
  <li>PEQA fine-tuning modifies only the quantization scale by:<br />
\(\widehat{\mathbf{W}} = (\mathbf{s}_0 + \Delta s) \cdot \overline{\mathbf{W}}_0 = (\mathbf{s}_0 + \Delta s) \cdot \left( \text{clamp} \left( \left\lfloor \frac{\mathbf{W}_0}{\mathbf{s}_0} \right\rfloor + \mathbf{z}_0, 0, 2^b - 1 \right) - \mathbf{z}_0 \right)\)
where \(\overline{\mathbf{W}}_0\) is frozen, and \(\Delta \mathbf{s} \in \mathbb{R}^{n \times 1}\) represents the gradient update of \(\mathbf{s}_0\) obtained by adaptation to a downstream task. 
<br /></li>
</ul>

<h2 id="conclusion">Conclusion</h2>
<p>PEQA presents a <strong>memory-efficient fine-tuning</strong> approach for quantized LLMs (weight-only quantization). By updating only the quantization scales while keeping integer matrices fixed, PEQA achieves:</p>
<ul>
  <li><strong>Comparable accuracy to full-precision PEFT methods</strong></li>
  <li><strong>Significant memory savings (up to 4× reduction)</strong></li>
  <li><strong>Seamless adaptation to new tasks</strong></li>
  <li><strong>Faster inference without additional post-processing</strong></li>
</ul>

<p>PEQA enables <strong>scalable and efficient model adaptation</strong> for large-scale language models, ensuring practical deployment on memory-constrained devices.</p>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="llm" /><category term="quantization" /><category term="finetuning" /><summary type="html"><![CDATA[Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization]]></summary></entry><entry><title type="html">HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs</title><link href="/blog/2025/halo-summary/" rel="alternate" type="text/html" title="HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs" /><published>2025-02-16T00:00:00+00:00</published><updated>2025-02-16T00:00:00+00:00</updated><id>/blog/2025/halo-summary</id><content type="html" xml:base="/blog/2025/halo-summary/"><![CDATA[<style>
li {
    font-size: 1.1rem; /* Adjust as needed */
}
</style>

<h1 id="halo-hadamard-assisted-lower-precision-optimization-for-llms"><a href="https://arxiv.org/abs/2501.02625">HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs</a></h1>
<blockquote>
  <p>[TL;DR] 
The paper proposes a quantized fine-tuning framework that exploits low bit-width matrix multiplication hardware units by performing quantization online for the inputs, weights, outputs, and their gradients with Hadamard transforms.</p>
</blockquote>

<h2 id="highlights">Highlights</h2>
<ul>
  <li>Provide and experiment with different settings: <a href="#halo-levels">HALO-0, HALO-1, HALO-2. Speedup: HALO-0 &gt; HALO-1 &gt; HALO-2, Accuracy: HALO-2 &gt; HALO-1 &gt; HALO-0</a></li>
  <li>Support and experiment with different data types: FP8, INT8, and FP6 for forward and backward passes, exploiting acceleration from <em>low bit-width matrix multiplication</em> hardware units</li>
  <li>Support Fully Sharded Data Parallel (FSDP) scheme to enable further savings by performing <em>low-precision communication</em></li>
  <li>Support full fine-tuning (FFT) and parameter-efficient fine-tuning (PEFT) methods such as LoRA</li>
  <li>Demonstrate accuracy improvements on downstream tasks for LLAMA-family models in quantized fine-tuning</li>
  <li>Demonstrate practical speedups in both FFT and FSDP cases</li>
  <li>Will open-source the kernel implementation at <a href="https://github.com/IST-DASLab/HALO">https://github.com/IST-DASLab/HALO</a></li>
</ul>

<h2 id="summary">Summary</h2>
<ul>
  <li><strong>Observation 1</strong>: The quantization errors of the forward activations deviate the gradient direction (compare Figures (b) and (c)).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_gradient_cosine-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_gradient_cosine-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_gradient_cosine-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_gradient_cosine.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>
<ul>
  <li><strong>Observation 2</strong>: The outliers of the gradient with respect to the layer output \(\mathbf{E}_{\mathbf{Y}}\) can only be eliminated by a <a href="#hadamard-transforms">left-hand Hadamard transformation</a> (see the figure on the right).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_lefthand_had-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_lefthand_had-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_lefthand_had-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_lefthand_had.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>
<ul>
  <li><strong>The problem statement</strong>: the large outliers in forward activations are difficult to represent in low bit-width data types.</li>
  <li><strong>The solution</strong>: The paper addresses this issue by transforming the forward activations online into a smoother space using Hadamard matrices.</li>
  <li><strong>The quantized fine-tuning framework</strong>: The proposed method quantizes the transformed inputs, weights, outputs, and their gradients with low bit-width data types (e.g., FP8, INT8, and FP6) in an online fashion to leverage acceleration from low bit-width matrix multiplication hardware units for forward and backward passes.</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>
<ul>
  <li><strong>Different strategies</strong>: The paper studies the placement of Hadamard rotations in both forward and backward passes. The left-hand Hadamard transformation (in the red rectangle) is applied to \(\mathbf{E}_{\mathbf{Y}}\).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_settings-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_settings-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_settings-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_settings.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><br /></p>

<h2 id="experiments">Experiments</h2>

<h4 id="compared-with-baselines">Compared with Baselines</h4>
<ul>
  <li>Left: Accuracy, Right: Relative speedup</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_accuracy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_accuracy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_accuracy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_vs_baselines_accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_speedups-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_speedups-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_vs_baselines_speedups-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_vs_baselines_speedups.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h4 id="halo-levels">HALO Levels</h4>
<ul>
  <li>Accuracy</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_level_accuracy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_level_accuracy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_level_accuracy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_level_accuracy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<ul>
  <li>Relative speedup</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/halo/Halo_level_speedups-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/halo/Halo_level_speedups-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/halo/Halo_level_speedups-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/halo/Halo_level_speedups.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p><br /></p>
<h2 id="notations">Notations</h2>

<h4 id="the-forward-and-backward-passes">The forward and backward passes</h4>
<ul>
  <li>Let matrices \(\mathbf{X} \in \mathbb{R}^{b \times m}\), \(\mathbf{W} \in \mathbb{R}^{n \times m}\), and \(\mathbf{Y} \in \mathbb{R}^{b \times n}\) be the inputs, weights, and outputs with batch size \(b\).</li>
  <li>\(\mathbf{E}_{\mathbf{X}}\), \(\mathbf{G}\), and \(\mathbf{E}_{\mathbf{Y}}\) are the gradients w.r.t. the inputs, weights, and outputs, respectively. The authors also refer \(\mathbf{E}_{\mathbf{X}}\) and \(\mathbf{E}_{\mathbf{Y}}\) as <strong>errors</strong>.</li>
  <li>The forward and backward calculations are defined as
\(\begin{align}
  \mathbf{Y} &amp;= \mathbf{X} \cdot \mathbf{W}^{\top} \quad &amp;\textbf{(Forward)} \\
  \mathbf{G} &amp;= \mathbf{E}_{\mathbf{Y}}^{\top} \cdot \mathbf{X} \quad &amp;\textbf{(Gradient)} \\
  \mathbf{E}_{\mathbf{X}} &amp;= \mathbf{E}_{\mathbf{Y}} \cdot \mathbf{W} \quad &amp;\textbf{(Error)}
\end{align}\)</li>
</ul>

<h4 id="quantization">Quantization</h4>
<ul>
  <li>Given half-precision (FP16) matrices \(\mathbf{A} \in \mathbb{R}^{m \times k}\) and \(\mathbf{B} \in \mathbb{R}^{k \times n}\)</li>
  <li>\(\mathbf{A}_Q\), \(\mathbf{B}_Q\) are the quantized copies of \(\mathbf{A}\) and \(\mathbf{B}\).</li>
  <li>Low precision matrix multiplication \(\mathbf{Y} = \mathbf{A_Q B_Q}\)</li>
</ul>

<h4 id="hadamard-transforms">Hadamard Transforms</h4>
<ul>
  <li>No Hadamard Case: \(\mathbf{Y} = \mathbf{A_Q B_Q}\)</li>
  <li>Left Case: \(\require{mathtools}\prescript{\mathbf{H}}{}{\mathbf{Y}} = \mathbf{H_m  (H_m^T A)_Q B_Q }\)</li>
  <li>Right Case: \(\mathbf{Y^H} = \mathbf{ A_Q (B H_n)_Q H_n^T}\)</li>
  <li>Middle Case: \(\stackrel{\mathbf{H}}{\mathbf{Y}} = \mathbf{(AH_k)_Q(H_k^TB)_Q}\)</li>
</ul>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="llm" /><category term="quantization" /><category term="training" /><summary type="html"><![CDATA[HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs]]></summary></entry><entry><title type="html">Solving `ptrace: Operation not permitted.` for GDB</title><link href="/blog/2024/ptrace-not-permitted/" rel="alternate" type="text/html" title="Solving `ptrace: Operation not permitted.` for GDB" /><published>2024-10-05T00:00:00+00:00</published><updated>2024-10-05T00:00:00+00:00</updated><id>/blog/2024/ptrace-not-permitted</id><content type="html" xml:base="/blog/2024/ptrace-not-permitted/"><![CDATA[<h1 id="the-permission-error">The Permission Error</h1>

<p>If  you see this error when attaching the GDB to a process:</p>
<blockquote>
  <p>ptrace: Operation not permitted.</p>
</blockquote>

<h1 id="the-solution">The Solution</h1>

<ol>
  <li>(Temporarily, sudo required) run <code class="language-plaintext highlighter-rouge">echo "0"|sudo tee /proc/sys/kernel/yama/ptrace_scope</code></li>
  <li>(Permanently, sudo required) editing the file <code class="language-plaintext highlighter-rouge">/etc/sysctl.d/10-ptrace.conf</code> and change the line: <code class="language-plaintext highlighter-rouge">kernel.yama.ptrace_scope = 1</code> to <code class="language-plaintext highlighter-rouge">kernel.yama.ptrace_scope = 0</code></li>
</ol>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://stackoverflow.com/questions/19215177/how-to-solve-ptrace-operation-not-permitted-when-trying-to-attach-gdb-to-a-pro">https://stackoverflow.com/questions/19215177/how-to-solve-ptrace-operation-not-permitted-when-trying-to-attach-gdb-to-a-pro</a></li>
  <li><a href="https://unix.stackexchange.com/questions/329504/proc-sys-kernel-yama-ptrace-scope-keeps-resetting-to-1">https://unix.stackexchange.com/questions/329504/proc-sys-kernel-yama-ptrace-scope-keeps-resetting-to-1</a></li>
</ul>]]></content><author><name></name></author><category term="linux" /><category term="ubuntu" /><category term="commands" /><summary type="html"><![CDATA[Solving `ptrace: Operation not permitted.` for GDB]]></summary></entry><entry><title type="html">Solving Nsight Compute Permission Error</title><link href="/blog/2024/nsight-compute-permission-error/" rel="alternate" type="text/html" title="Solving Nsight Compute Permission Error" /><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>/blog/2024/nsight-compute-permission-error</id><content type="html" xml:base="/blog/2024/nsight-compute-permission-error/"><![CDATA[<h1 id="the-permission-error">The Permission Error</h1>
<p>If you are running into this error message when using <a href="https://developer.nvidia.com/nsight-compute">Nsight-Compute</a></p>

<blockquote>
  <p>The user does not have permission to access NVIDIA GPU Performance Counters on the target device 0. For instructions on enabling permissions and to get more information see https://developer.nvidia.com/ERR_NVGPUCTRPERM</p>
</blockquote>

<h1 id="the-solution">The Solution</h1>

<ol>
  <li>(sudo required) Create <code class="language-plaintext highlighter-rouge">.conf</code> file (e.g. <code class="language-plaintext highlighter-rouge">profile.conf</code>) in folder <code class="language-plaintext highlighter-rouge">/etc/modprobe.d</code></li>
  <li>(sudo required) Open file <code class="language-plaintext highlighter-rouge">/etc/modprobe.d/profile.conf</code> in any editor</li>
  <li>(sudo required) Add below line in profile.conf <code class="language-plaintext highlighter-rouge">options nvidia NVreg_RestrictProfilingToAdminUsers=0</code>, and close file <code class="language-plaintext highlighter-rouge">/etc/modprobe.d/profile.conf</code></li>
  <li>(Optional) On some systems (or when using a package manager to install), it may be necessary to rebuild the initrd after writing a configuration file to <code class="language-plaintext highlighter-rouge">/etc/modprobe.d</code>. For RedHat-based distributions, rebuild the initrd with <code class="language-plaintext highlighter-rouge">dracut -–regenerate-all -f</code></li>
  <li>(sudo required) Restart your machine</li>
  <li>(Optional) Check that the value is correctly set to 0: <code class="language-plaintext highlighter-rouge">cat /proc/driver/nvidia/params | grep RmProfilingAdminOnly</code>, You should see: <code class="language-plaintext highlighter-rouge">RmProfilingAdminOnly: 0</code></li>
</ol>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters#AllUsersTag">https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters#AllUsersTag</a></li>
  <li><a href="https://forums.developer.nvidia.com/t/nvprof-warning-the-user-does-not-have-permission-to-profile-on-the-target-device/72374/4">https://forums.developer.nvidia.com/t/nvprof-warning-the-user-does-not-have-permission-to-profile-on-the-target-device/72374/4</a></li>
</ul>]]></content><author><name></name></author><category term="linux" /><category term="ubuntu" /><category term="nvidia" /><category term="commands" /><summary type="html"><![CDATA[Solving Nsight Compute Permission Error]]></summary></entry><entry><title type="html">The Academic Submission Journey</title><link href="/blog/2024/paper-submit-mindset/" rel="alternate" type="text/html" title="The Academic Submission Journey" /><published>2024-08-11T00:00:00+00:00</published><updated>2024-08-11T00:00:00+00:00</updated><id>/blog/2024/paper-submit-mindset</id><content type="html" xml:base="/blog/2024/paper-submit-mindset/"><![CDATA[<style>
/* The switch - the box around the slider */
.switch {
  position: relative;
  display: inline-block;
  width: 60px;
  height: 34px;
}

/* Hide default HTML checkbox */
.switch input {
  opacity: 0;
  width: 0;
  height: 0;
}

/* The slider */
.slider {
  position: absolute;
  cursor: pointer;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: #ccc;
  -webkit-transition: .4s;
  transition: .4s;
}

.slider:before {
  position: absolute;
  content: "";
  height: 26px;
  width: 26px;
  left: 4px;
  bottom: 4px;
  background-color: white;
  -webkit-transition: .4s;
  transition: .4s;
}

input:checked + .slider {
  background-color: #2196F3;
}

input:focus + .slider {
  box-shadow: 0 0 1px #2196F3;
}

input:checked + .slider:before {
  -webkit-transform: translateX(26px);
  -ms-transform: translateX(26px);
  transform: translateX(26px);
}

/* Rounded sliders */
.slider.round {
  border-radius: 34px;
}

.slider.round:before {
  border-radius: 50%;
}
</style>

<!-- Rounded switch -->
<p>English
<label class="switch">
  <input type="checkbox" id="language-toggle" />
  <span class="slider round"></span>
</label>
中文</p>

<div id="content-en" class="language-content">
  <!-- English content -->
<p>
I summarize some experience from paper submissions, and perhaps there are some things worth writing down to constantly remind myself or might be useful for the people who are submitting papers to conferences or journals.
</p>

<p>
The following text is from the perspective of authors who submit their papers to conferences or journals.
</p>

<h2> Summarizing Past Submission Experiences </h2>
<p><b> The key to acceptance: Novelty, Paper Organization, Experimental Results, and Most Importantly: Luck </b></p>
<p>
Whether a research paper can be accepted by a conference or journal depends not only on the integrity and innovation of the experiments but also on the tastes of the reviewers encountered during submission. Truly innovative and groundbreaking research is not common — there might only be a few particularly pioneering studies each year. Most researchers work within the existing framework, attempting to make limited breakthroughs from different angles. While becoming a top researcher may require talents, the skill of "writing papers" can be cultivated through effort. The key is whether the logic of the paper's argument and the presentation of experiments can guide reviewers to analyze the issue from the paper's perspective during their reading. Often, a paper that doesn't seem fully complete to oneself can be appreciated by a reviewer who provides suggestions for improving the research. With good luck, after supplementing the experiments, the paper might eventually be accepted by a journal or conference. Even if the final result is rejection, the submission process will make the research increasingly complete, and upon resubmission, it will likely be accepted by some journal or conference.
</p>

<blockquote>
Fun facts: Papers that I considered subpar inexplicably got accepted. However, papers that I particularly liked was harshly criticized by reviewers and repeatedly rejected.
</blockquote>

<h2> Conveying an Idea in a Paper </h2>
<p>
<b>Presenting "One" Perspective in a Paper with Extensive Argumentation and Evidence</b>
</p>

<p>
Conveying an idea with others is something that requires a lot of practice over a long period and is by no means easy, especially in situations where there is two-way, intensive, and opposing opinions. Practicing communication through writing is a relatively simple, controllable, and trainable process because, in writing, there is time to organize emotions, sort out logic, cite and provide evidence, and edit and revise. The basic principle of writing a paper is to provide sufficient context, argue for "one" perspective within that context, and then design experiments to support that "one" perspective. Exceeding "one" perspective can easily lead to ineffective communication, goal divergence, and a lack of focus throughout the paper. The designed experiments and figures must clearly and directly convey the argument, organizing the message into the most digestible form, such as charts or tables, to avoid logical twists. More importantly, it's crucial to think from the reader's standpoint — what kind of information do they want to receive? In this world, no one besides oneself knows this research better. The ability to organize research results into a form that people from different standpoints can understand is an important skill that can also be widely applied in the workplace.
</p>

<blockquote>
Fun facts: During the process of practicing paper writing, I found that communication with the opposite sex also became much smoother.
</blockquote>

<h2> Review and Rebuttal </h2>

<p>
<b>Preparing the Rebuttal in Advance and Setting the Debate Focus</b>
</p>

<p>
When submitting a research work, based on experience and understanding of the research, one would prepare for some questions that might be asked, setting up the main points of defense for each issue in advance. Usually, after preparation, one finds that the suggestions and questions from the reviewers mostly fall within the anticipated range. At this stage, finding papers that support your argument, supplementing experiments, and guiding the reviewer back to the pre-set focal points for debate based on existing literature and experimental results are essential. The goal is to refute the reviewers' viewpoints or alleviate their concerns, hoping that they will raise the paper's score, increasing the chances of it being accepted by a journal or conference.
</p>


<blockquote>
Fun facts: After so many submissions, my colleagues, my advisor, and I still feel frustrated when reading reviewers' comments.
</blockquote>

<h2> Waiting for the Notification </h2>

<p>
<b>Managing Expectations, Hoping for the Best, and Preparing for the Worst</b>
</p>

<p>
After debating with the reviewers, the next step is to wait for the final review results. This period is the most nerve-wracking, especially when the scores are borderline. It's essential to practice letting go of the debate with the reviewers and returning to normal life. Going out for a walk and staying away from research for a few days is usually a good approach. Once you feel slightly recovered, take some time to organize and revise the paper, incorporating the reviewers' doubts into the main text or appendix of the paper. While remaining optimistic, also look into other journals or conferences for potential resubmission options in case of the worst outcome.
</p>

</div>

<div id="content-zh" class="language-content" style="display:none;">
  <!-- Chinese content -->

<p>
我念博士班到目前為止，累積了一些投稿的經驗，也許有些東西可以寫下來，可以不斷的提醒自己，可能會對一些正在投稿的人也有幫助。
</p>

<p>
下面的文字是從一個投稿者的角度出發。
</p>

<h2>過往的投稿經驗</h2>
<p>
<b>論文的論述邏輯、實驗呈現與最重要的：運氣</b>
</p>

<p>
一個研究能不能登上會議或期刊，除了自身的實驗完整性、創新程度外，與當次投稿碰到的評審品味有相當大的關係。很創新、很突破的研究通常不是常態，一年可能只有幾篇特別開創性的研究。大多數的研究員都是在現有的架構下，嘗試去做不同角度、有限度的突破。成為頂尖的研究員也許很需要天賦，但「寫論文」的技能是花心思就可以被訓練出來的。關鍵在於，論文論述的邏輯與實驗的呈現，能不能引導評審在讀論文的過程中，去站在論文的角度去頗析這個問題。常常自己覺得不是那麼完整的論文，如果碰到能夠欣賞的評審，能夠提出完善這個研究的建議。運氣好，補充完實驗後最終會被接收入期刊或會議；即便最終結果被拒絕，投稿的過程會使研究越來越完整，再轉投稿，最終都能夠登上某些期刊或會議。
</p>

<blockquote>
Fun facts: 常常自己覺得很爛的論文，可以莫名其妙投上。自己覺得特別喜歡的作品，反而被評審砲轟，屢遭拒稿。
</blockquote>


<h2> 論文的論述與實驗</h2>

<p>
<b>呈現「一個」觀點，大量論述與佐證</b>
</p>

<p>
與人傳達想法不是一件簡單的事情，需要大量且長時間練習，相當需要溝通技巧。尤其是在雙向、密集且意見相佐的情況下。練習用文字溝通，是一個相對簡單可控、可訓練的過程，因為在寫作的過程有時間整理情緒、整理邏輯、引用與佐證、編輯與刪改。論文寫作最基本的原則是，給出足夠的情境，在情境下去論述「一個」自己的觀點，再設計實驗佐證自己的這「一個」觀點。超過「一個」觀點容易導致的無效溝通、目標發散，整篇論文看起來沒有重點。設計的實驗與圖必須要簡單直接的傳達論點，把想傳達的訊息，用圖或是表格整理成最容易攝取的形式，避免邏輯上的轉彎。更重要的是，學會去站在讀者的立場，去思考他們想要得到什麼樣的訊息？這個世界上，除了自己外，沒有其他人更清楚了解這個研究。把一個研究結果，整理成不同立場的人都可以理解形式，是一個重要且可以廣泛運用到職場的能力。
</p>

<blockquote>
Fun facts: 在練習寫論文過程中，我發現跟異性的溝通也變順暢了不少。
</blockquote>

<h2> 辯論階段（Review and Rebuttal）</h2>

<p>
<b>提前充分準備並設定辯論主軸</b>
</p>

<p>
投稿一個研究作品，依照經驗與對這個研究的了解，會先準備一些可能會被問到的問題，預先設定在每個議題上的攻防主軸。通常在準備後，會發現得到的評審建議與問題大部分會落在自己設定的範圍內。在這個階段，找到能佐證自己論點的論文、補充實驗，按照現有的文獻與手上的實驗結果，引導評審回到自己預先設定的主軸上去辯論，嘗試反駁評審的觀點或是弭平評審的疑慮，最終希望他們可以提高論文的分數，增加被期刊或會議接收的機率。
</p>

<blockquote>
Fun facts: 經歷這麼多次的投稿，看到評審建議，我和同事與我的指導教授依然會感到沮喪。
</blockquote>


<h2> 等待結果 </h2>

<p>
<b>管理期望，保持最好的期待，做最壞的打算</b>
</p>

<p>
與評審辯論後，接下來的時間就是等待最終的評審結果。這個時候也是最忐忑的，尤其是分數不上不下。練習放下跟評審的爭論，回歸自己正常的生活。出門走走，遠離研究個幾天通常是不錯的方式。等到自己狀態稍微恢復，花點時間整理修改論文，把評審提出的質疑整理進去論文的正文當中，或是整理進去附錄。保持樂觀之外，也查找看看在公佈結果後，有沒有可以轉投的期刊或會議，做最壞的打算。
</p>


</div>

<script>
document.getElementById('language-toggle').addEventListener('change', function() {
  if(this.checked) {
    document.getElementById('content-en').style.display = 'none';
    document.getElementById('content-zh').style.display = 'block';
  } else {
    document.getElementById('content-en').style.display = 'block';
    document.getElementById('content-zh').style.display = 'none';
  }
});
</script>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[The Academic Submission Journey]]></summary></entry><entry><title type="html">HiPPO Matrices</title><link href="/blog/2024/hippo_matrices/" rel="alternate" type="text/html" title="HiPPO Matrices" /><published>2024-07-11T00:00:00+00:00</published><updated>2024-07-11T00:00:00+00:00</updated><id>/blog/2024/hippo_matrices</id><content type="html" xml:base="/blog/2024/hippo_matrices/"><![CDATA[<style>
    #tableOfContents {
        font-size: 1.6em;
    }
</style>

<details style="background-color: #F5F5F5;">
<summary id="tableOfContents">Table of Contents</summary>
<ul style="font-size:1.4em">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#derive-hippo-matrices">Derive HiPPO Matrices</a></li>
    <ul>
        <li><a href="#backgrouds">Backgrounds</a></li>
        <ul>
            <li><a href="#approximation">Approximation</a></li>
            <li><a href="#orthogonal-polynomial-basis">Orthogonal Polynomial Basis</a></li>
            <li><a href="#tilted-measure-and-basis">Tilted Measure and Basis</a></li>
        </ul>
        <li><a href="#the-projection-and-coefficients">The Projection and Coefficients</a></li>
        <ul>
            <li><a href="#approximate-input-function">Approximate Input Function</a></li>
            <li><a href="#reconstruct-input-function">Reconstruct Input Function</a></li>
            <li><a href="#coefficient-dynamics">Coefficient Dynamics</a></li>
        </ul>
        <li><a href="#case-hippo-legs">Case: HiPPO-LegS</a></li>
        <ul>
            <li><a href="#properties-of-legendre-polynomials">Properties of Legendre Polynomials</a></li>
            <li><a href="#shifted-and-scaled-legendre-polynomials">Shifted and Scaled Legendre Polynomials</a></li>
            <li><a href="#derivatives-of-legendre-polynomials">Derivatives of Legendre Polynomials</a></li>
            <li><a href="#derive-hippo-legs">Derive HiPPO-LegS</a></li>
        </ul>
    </ul>
  <li><a href="#references">References</a></li>
</ul>
</details>

<p><br /></p>

<h1 id="introduction">Introduction</h1>
<p>This document provides the mathematic derivations of the HiPPO matrices that help readers understand the formulas. The contents are extracted and summarized from the original papers cited in <a href="#references">references</a>.</p>

<p><br /></p>

<h1 id="derive-hippo-matrices">Derive HiPPO Matrices</h1>

<p>The contents are extracted and summarized from the original paper <a href="#1">[1]</a>, but we show more details in the derivations to help people to understand.</p>

<p><br /></p>

<h2 id="backgrounds">Backgrounds</h2>

<p>HiPPO considers a linear time-invariant (LTI) ODEs such that</p>

\[\Large
\begin{aligned}
\dot{c}(t)=Ac(t)+Bf(t) .
\end{aligned}\]

<ol>
  <li>
    <p>\(c(t)\) is the coefficient vector that describes the combination of
the basis functions, which corresponds to the state vector \(h(t)\) in the continuous state space
model.</p>
  </li>
  <li>
    <p>\(f(t)\) is the input signal that corresponds to the input vector \(x(t)\) in the continuous state
space model.</p>
  </li>
  <li>
    <p>\(\dot{c}(t)\) is the system dynamics that represents a compression of the history of \(f\) that satisfies
linear dynamics.</p>
  </li>
</ol>

<p><br /></p>
<h4 id="approximation">Approximation</h4>

<p>Given a time-varying measure family \(u^{(t)}\) supported on \(( -\inf, t]\),  a sequence of basis functions \(\mathcal{G} = \text{span} \{ {g_n}^{(t)} \}_{n \in [N]}\)]
, and a continuous function \(f: \mathbb{R}_{\geq0} \mapsto \mathbb{R}\), HiPPO defines an operator that maps \(f\) to the optimal projection coefficients \(c: \mathbb{R}_{\geq0}\mapsto \mathbb{R}^N\), such that</p>

\[\Large
\begin{aligned}
    g^{(t)} = \text{argmin}_{g \in \mathcal{G}}\left\lVert f_{x \leq t} - g\right\rVert_{\mu^{(t)}}
\end{aligned}

\quad  \text{and} \quad

\Large
\begin{aligned}
    g^{(t)} = \sum^{N-1}_{n=0} c_n(t) g_n^{(t)}
\end{aligned}\]

<p><br /></p>
<h4 id="measures">Measures</h4>

<p>At every \(t\), the approximation quality is defined with respect to a measure \(\mu^{(t)}\) supported on \((-\infty, t]\). We seek some polynomial \(g^{(t)}\) of degree at most \(N-1\) that minimizes the error \(\begin{aligned}
    \left\lVert f_{x \leq t} - g^{(t)}\right\rVert_{L_2(\mu^{(t)})}
\end{aligned}\).
Suppose the measures \(\mu^{(t)}\) are sufficiently smooth across their domain as well as in time and have desities
\(\begin{aligned}
\omega(t, x) = \frac{d\mu^{(t)}}{d\lambda}(x)
\end{aligned}\)
with respect to the Lebesgue measure \(d\lambda(x)=dx\)
such that \(\omega\) is \(C^1\) almost everywhere.
Therefore, we have 
\(\begin{aligned}
\omega(t, x) = \frac{d\mu^{(t)}(x)}{dx}
\end{aligned}\).
Thus, integrating against
\(d\mu^{(t)}(x)\) can be rewritten as integrating against
\(\omega(t, x)dx\). 
That is
\(\begin{aligned}
\int d\mu^{(t)}(x) = \int \omega(t, x)dx
\end{aligned}\).</p>

<p><br /></p>
<h4 id="orthogonal-polynomial-basis">Orthogonal Polynomial Basis</h4>

<p>We can use a sequence of orthogonal polynomials (OPs) as our basis to approximate the continuous function \(f\).
A sequence of OPs \(P_0(x), P_1(x), ...\) satisfying:</p>

<ol>
  <li>
    <p>\(\text{deg}(P_i)=i\), and</p>
  </li>
  <li>
    <p>\(\langle P_i, P_j \rangle = \int P_i(x) P_j(x) d\mu(x) = 0\) for all \(i \neq j\)</p>
  </li>
</ol>

<p>A sequence of OPs \(g = \{P_n\}_{n \in \mathbb{N}}\) of degree \(\text{deg}(g)&lt;N\) that approximate a function \(f\) is
then given by</p>

\[\begin{aligned}
    &amp; \sum_{i=0}^{N-1} c_i \frac{P_i(x)}{\left\lVert P_i\right\rVert^2_\mu} \quad \text{where } c_i = \langle f, P_i \rangle = \int f(x)P_i(x)d\mu(x) \\
\end{aligned}.\]

<p>This projects an input sequence signal onto a sequence of OPs, and \(c_i\) describes the magnitude of
the \(P_i\) to reconstruct the input signal.</p>

<p>Let \(\{P_n\}_{n \in \mathbb{N}}\) denote a sequence of OPs with respect
to some base measure \(\mu\).
Similarly, define
\(\{ {P_n}^{(t)} \}_{n \in \mathbb{N}}\) 
to be a sequence of OPs with
respect to the time-varying measure \(\mu^{(t)}\). 
Let \({p_n}^{(t)}\) be the normalized version of \({P_n}^{(t)}\) (<em>i.e.,</em> have norm 1), and define
\(\begin{aligned}
p_n(t, x) = {p_n}^{(t)}(x)
\end{aligned}\).
The \({P_n}^{(t)}\) are not required to be normalized, while the
\({p_n}^{(t)}\) are.</p>

<p><br /></p>
<h4 id="tilted-measure-and-basis">Tilted Measure and Basis</h4>
<p>The goal is simply to store a compressed representation of functions, which
can use any basis, not necessarily OPs. For any scaling function
\(\begin{aligned}
    \chi(t, x) = \chi^{(t)}(x)
\end{aligned}\)
such that \(p_n(x)\chi(x)\) are orthogonal with respect to
the density \(\omega/\chi^2\) at every time \(t\).
Thus, we can choose this alternative basis and measure to perform the projections.</p>

<p>Define \(\nu\) to be the <code class="language-plaintext highlighter-rouge">normalized measure</code> with density proportional to
\(\omega^{t}/(\chi^{(t)})^2\).
We will calculate the normalized measure and the orthonormal basis for it.</p>

<p>Let \(\zeta(t)\) to be the normalization
constant: \(\begin{aligned}
    \zeta(t) = \int \frac{\omega}{\chi^2} = \int \frac{\omega^{(t)}(x)}{({\chi^{(t)}(x)})^2}dx
\end{aligned}\) , so that \(\nu^{(t)}\) has density
\(\frac{\omega^{(t)}(x)}{\zeta(t)(\chi^{(t)}(x))^2}\)
If \(\chi(t, x) = 1\) (no tilting), this constant is \(\chi(t) = 1\).</p>

<p>Note that (dropping the dependence on x inside the integral for shorthand)</p>

\[\begin{aligned}
\left\lVert \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)}\right\rVert_{\nu^{(t)}}^2 &amp;= \int {( \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)})}^2 \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\
&amp;= \int (p_n^{(t)})^2 \omega^{(t)} \\
&amp;= \left\lVert p_n^{(t)}\right\rVert_{\mu^{(t)}}^2 = 1 \quad.
\end{aligned}\]

<p>Thus we define the <code class="language-plaintext highlighter-rouge">orthogonal basis</code> for \(\nu^{(t)}\) (normalized measurement)</p>

\[\Large
\label{orthogonal_basis}
\tag{eq:1}
\begin{aligned}
    {g_n}^{(t)} = \lambda_n \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)}, \quad n \in \mathbb{N}
\end{aligned}\]

<p>Therefore, we have</p>

\[\Large
\label{gn_gm}
\tag{eq:2}
\begin{aligned}
\langle {g_n}^{(t)}, {g_m}^{(t)} \rangle_{\nu^{(t)}} = \lambda_n^2 \delta_{n,m} \quad.
\end{aligned}\]

<p>Note that when \(\lambda_n=\pm1\), the basis \(\{ {g_n}^{(t)} \}\) is an
orthonormal basis with respect to the measure \(\nu^{(t)}\), at every time
\(t\). Notationally, let \({g_n}^{(t)}(x)=g_n(t, x)\) as usual.</p>

<p><br /></p>

<h2 id="the-projection-and-coefficients">The Projection and Coefficients</h2>

<p>Given a choice of measures \(\nu\) and basis functions \(g\), we compute the
coefficients \(c(t)\) to approximate a \(C^1\)-smooth function which is seen
<em>online</em> \(f(x)_{x \leq t}\).</p>

<p><br /></p>

<h4 id="approximate-input-function">Approximate Input Function</h4>
<p>We project the input function \(f\) to basis functions \(g\), and \(c(t)\) is the coefficient vector that describes the magnitude of the basis functions.
Using the result from the formula <a href="#eq:1">[eq:1]</a>, we have</p>

\[\label{approx_f}
\tag{eq:3}
\begin{aligned}
    c_n(t) &amp;= \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} \\
    &amp;= \int f \textcolor{red}{ {g_n}^{(t)} }  \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\
    &amp;= \int f \textcolor{red} { \lambda_n \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)} } \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\
    &amp;= \int f \lambda_n \zeta(t)^{-\frac{1}{2}} {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}} \\
    &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}}
\end{aligned}\]

<p><br /></p>

<h4 id="reconstruct-input-function">Reconstruct Input Function</h4>
<p>The function \(f\) can be approximated by storing its coefficients with respect to the basis.
At any time \(t\), \(f(x)_{x \leq t}\) can be explicitly reconstructed by using \(c_n(t) = \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}}\) and the formula <a href="#eq:2">[eq:2]</a>, such that</p>

\[\label{reconstruct}
\tag{eq:4}
\begin{aligned}
    f(x)_{x \leq t} \approx {g}^{(t)} &amp;= \sum_{n=0}^{N-1} \textcolor{red} { \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} } \frac{ {g_n}^{(t)} }{ {\left \lVert {g_n}^{(t)} \right \rVert }_{\nu^{(t)}}^2} \\
    &amp;= \sum_{n=0}^{N-1} \textcolor{red}{c_n(t)} \frac{ {g_n}^{(t)} }{ \textcolor{green}{ {\left \lVert {g_n}^{(t)} \right \rVert }_{\nu^{(t)}}^2} } \\
    &amp;= \sum_{n=0}^{N-1} c_n(t) \frac{ {g_n}^{(t)} }{ \textcolor{green}{ \langle {g_n}^{(t)}, {g_n}^{(t)} \rangle_{\nu^{(t)}} } } \\
    &amp;= \sum_{n=0}^{N-1} c_n(t) \frac{ {g_n}^{(t)} }{ \textcolor{green}{\lambda_n^2} } \\
    &amp;= \sum_{n=0}^{N-1} \lambda_n^{-2} c_n(t) {g_n}^{(t)} \\
    &amp;= \sum_{n=0}^{N-1} \lambda_n^{-1} \zeta^{\frac{1}{2}} c_n(t) {p_n}^{(t)} \chi^{(t)} \\
\end{aligned}\]

<p><br /></p>

<h4 id="coefficient-dynamics">Coefficient Dynamics</h4>

<p>The coefficients \(c(t)\) encode information about the history of \(f\) and
allow online predictions. We compute these coefficients over time by
viewing them as a dynamical system. Differentiating \(c_n(t)\), we have</p>

\[\begin{aligned}
    \frac{d}{dt} c_n(t) &amp;= \frac{d}{dt}( \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} ) \\
    &amp;= \frac{d}{dt}( \zeta(t)^{-\frac{1}{2}} \lambda_n \int f \textcolor{red}{ {p_n}^{(t)} } \textcolor{green}{ \frac{\omega^{(t)}}{\chi^{(t)}} } ) \\
    &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f \textcolor{red}{ \frac{\partial}{\partial t}({p_n}^{(t)}) } \frac{\omega^{(t)}}{\chi^{(t)}} + \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)} \textcolor{green}{ \frac{\partial}{\partial t}(\frac{\omega^{(t)}}{\chi^{(t)}} ) } \\
    &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f(x) (\frac{\partial}{\partial t} p_n(t, x)) \frac{\omega}{\chi}(t, x) dx + \int f(x) (\zeta^{-\frac{1}{2}}\lambda_n  p_n(t, x)) (\frac{\partial}{\partial t} \frac{\omega}{\chi}(t, x)) dx \quad.
\end{aligned}\]

<p><br /></p>
<h2 id="case-hippo-legs">Case: HiPPO-LegS</h2>

<p><br /></p>
<h4 id="properties-of-legendre-polynomials">Properties of Legendre Polynomials</h4>

<p>An especially compact expression for the Legendre polynomials is given
by Rodrigues’ formula <a href="#4">[4]</a>:</p>

\[\begin{aligned}
P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n}(x^2 - 1)^n .
\end{aligned}\]

<p>The first few Legendre polynomials are:</p>

\[\begin{aligned}
P_0(x) &amp;= 1 \\
P_1(x) &amp;= x \\
P_2(x) &amp;= \frac{1}{2} (3x^2 - 1) \\
P_3(x) &amp;= \frac{1}{2} (5x^3 - 3x) \\
\end{aligned}\]

<p>The Legendre polynomials are orthogonal over \((-1,1)\) with respect to
the measure \(\omega^{leg}=\boldsymbol{1}_{[-1, 1]}\) and they satisfy</p>

\[\label{legendre}
\tag{eq:5}
\begin{aligned}
    \frac{2n+1}{2}\int_{-1}^{1} P_n(x) P_m(x) dx = \delta_{mn}
\end{aligned}\]

<p>where \(\delta_{mn}\) is Kronecker delta, equal to \(1\) if \(m = n\) and to \(0\) otherwise, such that</p>

\[\begin{aligned}
\delta_{mn} = 
    \begin{cases}
    0,&amp; \text{if } m \neq n \\
    1,&amp; \text{if } m = n \\
\end{cases}
\end{aligned}\]

<p>Also, they satisfy</p>

\[\begin{aligned}
    P_n(1) = 1 \\
    P_n(-1) = (-1)^n \\
\end{aligned}\]

<p>so called "Pointwise evaluations".</p>

<p><br /></p>
<h4 id="shifted-and-scaled-legendre-polynomials">Shifted and Scaled Legendre Polynomials</h4>

<p>The "shifted" Legendre polynomials are a set of functions analogous to
the Legendre polynomials, but defined on the interval \((0, 1)\) <a href="#5">[5]</a>. They obey
the orthogonality relationship</p>

\[\begin{aligned}
    (2n+1)\int_{0}^{1} P_n(x) P_m(x) dx = \delta_{mn}
\end{aligned}\]

<p>We will also consider scaling the Legendre polynomials to be orthogonal
on the interval \([0, t]\). Let \(u=\frac{2x}{t}-1\), and apply a change of variables from <a href="#eq:5">[eq:5]</a></p>

\[\begin{aligned}
    &amp; \frac{2n+1}{2}\int_{u=-1}^{u=1} P_n(u) P_m(u) du \\
    &amp; \text{since } u = \frac{2x}{t}-1 \text{ , we have } du = \frac{2}{t}dx  \\
    &amp; \text{since } x = \frac{t}{2}(u+1) \text{ , we have } x = 0 \text{ when } u=-1 \text{ ,and } x=t \text{ when } u=1  \\
    &amp; \text{replace } u \text{ with } x \\
    &amp;= \frac{2n+1}{2}\int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{2}{t}dx \\
    &amp;= (2n+1)\int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{t}dx \\
    &amp;= (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{2}\omega^{leg}(\frac{2x}{t}-1) \frac{1}{t} dx \\
    &amp;= \delta_{nm}
\end{aligned}\]

<p>Therefore, with respect to the measure \(\omega_t=\boldsymbol{1}_{[0, t]} / t\) (which is a probability measure for all t), we have</p>

\[\begin{aligned}
    &amp; (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{2}\omega^{leg}(\frac{2x}{t}-1) \frac{1}{t} dx \\
    &amp;= (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \omega_{t}(\frac{2x}{t}-1) dx \\
\end{aligned}\]

<p>Then, the normalized orthogonal polynomials in the interval \([0, t]\) with starting point is \(\textcolor{red}{0}\) and step size \(\textcolor{green}{t}\) are</p>

\[\begin{aligned}
    (2n+1)^{\frac{1}{2}} P_n(\frac{2x}{t}-1).
    \quad \left( =(2n+1)^{\frac{1}{2}} P_n(\frac{2(x-\textcolor{red}{0})}{\textcolor{green}{t}}-1) \right)
\end{aligned}\]

<p>Generalize to any interval \([t-\theta, t]\), we use \(\textcolor{red}{t-\theta}\) as the starting point and \(\textcolor{green}{\theta}\) as the step size. After replacing the notation, we have</p>

\[\begin{aligned}
    &amp;(2n+1)^{\frac{1}{2}} P_n(\frac{2(x - \textcolor{red}{ (t-\theta)}) }{\textcolor{green}{\theta}}-1) \\
    &amp;=(2n+1)^{\frac{1}{2}} P_n(\frac{2x-2t+2\theta}{\theta} - 1) \\
    &amp;=(2n+1)^{\frac{1}{2}} P_n(\frac{2(x-t)}{\theta} + 1) \\
\end{aligned}\]

<p>, which is orthonormal for the uniform measure
\(\omega_\theta= \frac{1}{ \theta}\boldsymbol{1}_{[t-\theta, t]}\)</p>

<p><br /></p>
<h4 id="derivatives-of-legendre-polynomials">Derivatives of Legendre Polynomials</h4>

<p>use the following recurrence relations (the integration of Legendre polynomial <a href="#4">[4]</a>)</p>

\[\label{legendre_recur_1}
\tag{eq:6}
\begin{aligned}
    (2n+1)P_n &amp;= P'_{n+1} - P'_{n-1}
\end{aligned}\]

<p>and</p>

\[\label{legendre_recur_2}
\tag{eq:7}
\begin{aligned}
    P'_{n+1} &amp;= (n+1)P_n + xP'_n
\end{aligned}\]

<p>The first equation <a href="#eq:6">[eq:6]</a> yields</p>

\[\begin{aligned}[c]
    P'_{n+1} &amp;= (2\mathbf{n}+1)P_\mathbf{n} + \textcolor{red}{P'_{n-1}} \\
    \textcolor{red}{P'_{n-1}} &amp;= (2(\mathbf{n-2})+1)P_\mathbf{n-2} + \textcolor{green}{P'_{n-3}} \\
    \textcolor{green}{P'_{n-3}} &amp;= (2(\mathbf{n-4})+1)P_\mathbf{n-4} + \textcolor{blue}{P'_{n-5}} \\
    ...
\end{aligned}
\quad\quad
\begin{aligned}[c]
    P'_{n} &amp;= (2(\mathbf{n-1})+1)P_\mathbf{n-1} + \textcolor{red}{P'_{n-2}} \\
    \textcolor{red}{P'_{n-2}} &amp;= (2(\mathbf{n-3})+1)P_\mathbf{n-3} + \textcolor{green}{P'_{n-4}} \\
    \textcolor{green}{P'_{n-4}} &amp;= (2(\mathbf{n-5})+1)P_\mathbf{n-5} + \textcolor{blue}{P'_{n-6}} \\
    ...
\end{aligned}\]

<p>Therefore we have</p>

\[\begin{aligned}
    &amp; P'_{n+1} = (2n+1)P_n + (2n-3)P_{n-2} + (2n-7)P_{n-4} ... \quad \text{ and} \\
    &amp; P'_n = (2n-1)P_{n-1} + (2n-5)P_{n-3} + (2n-9)P_{n-5} ...
\end{aligned}\]

<p>where the sum stops at \(P_0\) or \(P_1\). By summing \(P'_{n+1} + P'_n\), we have</p>

\[\begin{aligned}
    &amp; P'_{n+1} +  P'_n = \textcolor{red}{(2n+1)P_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\
    &amp; P'_{n+1} +  P'_n = \textcolor{red}{nP_n + (n+1)P_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \text{ (using eq:7)}\\
    &amp; P'_{n+1} +  P'_n = nP_n + \textcolor{green}{P'_{n+1} - xP'_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\
    &amp;  P'_n \textcolor{green}{+ xP'_n} = nP_n + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\
\end{aligned}\]

<p>Therefore, we have</p>

\[\label{legendre_result}
\tag{eq:8}
\begin{aligned}
    (x+1)P'_n(x) &amp;= nP_n + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\
\end{aligned}\]

<p>We will use the result <a href="#eq:8">[eq:8]</a> to derive HiPPO-LegS.</p>

<p><br /></p>
<h4 id="derive-hippo-legs">Derive HiPPO-LegS</h4>

<p>Now we have the measure \(\omega(t, x) = \frac{1}{t}\mathbb{1}_{[0,t]}\)
and the basis</p>

\[g_n(t, x) = p_n(t, x) = (2n+1)^{\frac{1}{2}}P_n(\frac{2x}{t}-1)\]

<p>where \(P_n\) are the basic Legendre polynomials.</p>

<p>Since the basis functions \(g_n(t, x)\) are orthonormal with respect to the measure, we have the tiling function
\(\chi(t, x) = 1\) (no tilting), \(\zeta(t, x) = 1\), \(\lambda_n=1\)</p>

<p>Plug \(\chi, \zeta, \lambda_n\) into <a href="#eq:3">[eq:3]</a>, we have</p>

\[\begin{aligned}
    c_n(t) &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}} =  \int f {p_n}^{(t)} \omega^{(t)}
\end{aligned}\]

<ul>
  <li><strong>HiPPO-LegS Derivatives</strong></li>
</ul>

<p>We first differentiate the measure and basis:</p>

\[\begin{aligned}
    \frac{\partial}{\partial t} \omega(t, \cdot) &amp;= -t^{-2} \mathbb{1}_{[0, t]} + t^{-1} \delta_t = t^{-1} (-\omega(t) + \delta_t) \\
    \frac{\partial}{\partial t} g_n(t, x) &amp;= -(2n+1)^{\frac{1}{2}}2xt^{-2} {P'}_n(\frac{2x}{t}-1) \\
    &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} (\textcolor{red}{\frac{2x}{t}-1}+1) {P'}_n(\textcolor{red}{\frac{2x}{t}-1}) \quad.
\end{aligned}\]

<p>Now define \(z = \frac{2x}{t} - 1\) and apply the
result of derivatives of Legendre polynomials in the equation <a href="#eq:8">[eq:8]</a>.</p>

\[\begin{aligned}
    \frac{\partial}{\partial t} g_n(t, x) &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} (z+1) {P'}_n(z) \\
    &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} [nP_n(z) + (2n-1)P_{n-1}(z) + (2n-3)P_{n-2}(z) + ...] \\
    &amp; \left( \text{using } \quad (2n+1)^{-\frac{1}{2}} g_n(t, x) = P_n(\frac{2x}{t}-1) = P_n(z)\right) \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} g_n(t, x) + (2n-1)^{\frac{1}{2}} g_{n-1}(t, x) + (2n-3)^{\frac{1}{2}}g_{n-2}(t, x) + ...] \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} g_n^{(t)} + (2n-1)^{\frac{1}{2}} g_{n-1}^{(t)} + (2n-3)^{\frac{1}{2}}g_{n-2}^{(t)} + ...] \\
\end{aligned}\]

<ul>
  <li><strong>HiPPO-LegS Coefficient Dynamics</strong></li>
</ul>

\[\begin{aligned}
    \frac{d}{dt} c_n(t) &amp;=  \frac{d}{dt} ( \int f {p_n}^{(t)} \omega^{(t)} dx) \\
    &amp;= \int f (\textcolor{red}{\frac{\partial}{\partial t} g_n^{(t)}}) \omega^{(t)} dx  + \int f g_n^{(t)} \textcolor{green}{\frac{\partial}{\partial t} \omega^{(t)}} dx \\
    
    &amp;= \int f \left( \textcolor{red}{ -t^{-1}(2n+1)^{\frac{1}{2}} \left[ n (2n+1)^{-\frac{1}{2}}   g_n^{(t)}  + (2n-1)^{\frac{1}{2}} g_{n-1}^{(t)} + ...\right] } \right) \omega^{(t)} dx
    + \left( \int f g_n^{(t)} \textcolor{green}{t^{-1} (-\omega^{(t)} + \delta_t)} dx \right)  \\

    &amp;= \left( -t^{-1}(2n+1)^{\frac{1}{2}} \left[n (2n+1)^{-\frac{1}{2}} \textcolor{red}{\int f g_n^{(t)} \omega^{(t)} dx}  + (2n-1)^{\frac{1}{2}} \textcolor{red}{\int f g_{n-1}^{(t)} \omega^{(t)} dx}  + ...\right] \right)
    + \left( \int f g_n^{(t)} t^{-1} (-\omega^{(t)} + \delta_t) dx \right)   \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} \textcolor{red}{c_n(t)} + (2n-1)^{\frac{1}{2}} \textcolor{red}{c_{n-1}(t)} + (2n-3)^{\frac{1}{2}}  \textcolor{red}{c_{n-2}(t)} + ...]
    + \left( \int f g_n^{(t)} t^{-1} (-\omega^{(t)} + \delta_t) dx \right)   \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}}  c_{n-2}(t) + ...]
     + \left( - t^{-1} \textcolor{green}{ \int f g_n^{(t)} \omega^{(t)}dx} +   t^{-1} \textcolor{green}{ \int f g_n^{(t)} \delta_t dx} \right)   \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}}  c_{n-2}(t) + ...]
    -t^{-1} \textcolor{green}{c_n(t)} + t^{-1} \textcolor{green}{f(t)  g_{n}^{(t)}(t)}  \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}}  c_{n-2}(t) + ...]
    -t^{-1} c_n(t) + t^{-1} \textcolor{green}{f(t)  (2n+1)^{\frac{1}{2}}P_n(1)}  \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}}  c_{n-2}(t) + ...]
    \textcolor{red}{-t^{-1}c_n(t)} + t^{-1} f(t) (2n+1)^{\frac{1}{2}}   \\
    &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [\textcolor{red}{(n+1)} (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}}  c_{n-2}(t) + ...]
    + t^{-1} f(t) (2n+1)^{\frac{1}{2}}   \\
\end{aligned}\]

<p>For example:</p>

\[\begin{aligned}
    \frac{d}{dt} c_0(t) &amp;= -\frac{1}{t} [(n+1) c_0(t)] = -\frac{1}{t} [1 c_0(t)] \\
    \frac{d}{dt} c_1(t) &amp;= -\frac{1}{t} [(n+1) c_1(t) + (2n+1)^{\frac{1}{2}}(2n-1)^{\frac{1}{2}}c_0(t)] = -\frac{1}{t} [2 c_1(t) + 3^{\frac{1}{2}}1^{\frac{1}{2}}c_0] \\
    \frac{d}{dt} c_2(t) &amp;= -\frac{1}{t} [(n+1) c_2(t) + (2n+1)^{\frac{1}{2}}(2n-1)^{\frac{1}{2}}c_1(t)+ (2n+1)^{\frac{1}{2}}(2n-3)^{\frac{1}{2}}c_0(t)] \\ 
    &amp; \quad = -\frac{1}{t} [3 c_2(t) + 5^{\frac{1}{2}}3^{\frac{1}{2}}c_1+ 5^{\frac{1}{2}}1^{\frac{1}{2}}c_0] \\
    &amp; ... \\
    \frac{d}{dt} c_n(t) &amp;= -\frac{1}{t} [(n+1) c_n(t) + \sum_{k=n-1}^{0}(2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}}c_k(t)] \\ 
\end{aligned}\]

<p>Writing the result into the matrix form:</p>

\[\begin{aligned}
    \frac{d}{dt} c_n(t) = -\frac{1}{t} A c(t) + \frac{1}{t} B f(t)
\end{aligned}\]

<p>where</p>

\[\begin{aligned}
A_{nk} &amp;= 
\begin{cases}
    (2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}},&amp; \text{if } n &gt; k \\
    n+1,&amp; \text{if } n = k \\
    0,&amp; \text{if } n &lt; k \\
\end{cases}
\\
B &amp;= (2n+1)^{\frac{1}{2}}
\end{aligned}\]

<p>We can also write the result into the form: \(\begin{aligned}
    \frac{d}{dt} c_n(t) = -\frac{1}{t} D[ MD^{-1}c(t) + \mathbb{1} f(t)]
\end{aligned}\)</p>

<p>where</p>

\[\begin{aligned}
D &amp;= \text{diag}[(2n+1)^{\frac{1}{2}}]_0^{n-1} \\
M_{nk} &amp;= 
\begin{cases}
    2k+1,&amp; \text{if } n &gt; k \\
    k+1,&amp; \text{if } n = k \\
    0,&amp; \text{if } n &lt; k \\
\end{cases}
\end{aligned}\]

<ul>
  <li><strong>HiPPO-LegS Reconstruction</strong></li>
</ul>

<p>Plug \(\chi, \zeta, \lambda_n\) into <a href="#eq:4">[eq:4]</a>, we have</p>

\[\begin{aligned}
    f(x)_{x \leq t} \approx {g}^{(t)} &amp;= \sum_{n=0}^{N-1} \lambda_n^{-1} \zeta^{\frac{1}{2}} c_n(t) {p_n}^{(t)} \chi^{(t)} \\
    &amp;= \sum_{n=0}^{N-1} c_n(t) {p_n}^{(t)} = \sum_{n=0}^{N-1} c_n(t) {g_n}(t, x)\\
    &amp;= \sum_{n=0}^{N-1} c_n(t) (2n+1)^{\frac{1}{2}}P_n(\frac{2x}{t}-1)
\end{aligned}\]

<p><br /></p>

<h1 id="references">References</h1>

<p><a id="1">[1]</a> 
A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré, “Hippo: Recurrent memory with optimal
polynomial projections,” Advances in Neural Information Processing Systems, vol. 33, pp. 1474–
1487, 2020.</p>

<p><a id="2">[2]</a> 
A. Gu, K. Goel, A. Gupta, and C. Ré, “On the parameterization and initialization of diagonal state
space models,” Advances in Neural Information Processing Systems, vol. 35, pp. 35971–35983,
2022.</p>

<p><a id="3">[3]</a> 
A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv
preprint arXiv:2312.00752, 2023.</p>

<p><a id="4">[4]</a> 
<a href="https://en.wikipedia.org/wiki/Legendre_polynomials">https://en.wikipedia.org/wiki/Legendre_polynomials</a></p>

<p><a id="5">[5]</a> 
<a href="https://mathworld.wolfram.com/LegendrePolynomial.html">https://mathworld.wolfram.com/LegendrePolynomial.html</a></p>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="algorithm" /><category term="SSM" /><summary type="html"><![CDATA[HiPPO Matrices]]></summary></entry><entry><title type="html">State Space Models</title><link href="/blog/2024/state_space_models/" rel="alternate" type="text/html" title="State Space Models" /><published>2024-07-11T00:00:00+00:00</published><updated>2024-07-11T00:00:00+00:00</updated><id>/blog/2024/state_space_models</id><content type="html" xml:base="/blog/2024/state_space_models/"><![CDATA[<style>
    #tableOfContents {
        font-size: 1.6em;
    }
</style>

<details style="background-color: #F5F5F5;">
<summary id="tableOfContents">Table of Contents</summary>
<ul style="font-size:1.4em">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#continuous-state-space-model">Continuous State Space Model</a></li>
  <li><a href="#discretize-continuous-state-space-model">Discretize Continuous State Space Model</a></li>
  <ul>
    <li><a href="#bilinear-transform">Bilinear Transform</a></li>
    <li><a href="#zero-order-hold-transform">Zero-order Hold Transform</a></li>
  </ul>
  <li><a href="#discrete-time-state-space-model">Discrete-time State Space Model</a></li>
  <ul>
    <li><a href="#convolution-form&lt;">Convolution Form</a></li>
    <li><a href="#parallel-associative-scan">Parallel Associative Scan</a></li>
  </ul>
  <li><a href="#references">References</a></li>
</ul>
</details>

<p><br /></p>

<h1 id="introduction">Introduction</h1>
<p>This document provides the mathematic derivations of the State Space Models that help readers understand the formulas. The contents are extracted and summarized from the original papers cited in <a href="#references">references</a>.</p>

<p><br /></p>

<h1 id="continuous-state-space-model">Continuous State Space Model</h1>

<p>Consider a continuous state space model equation:</p>

\[\label{c_ssm}
\tag{1}
\Large
\begin{aligned}
\dot{h}(t)=Ah(t)+Bx(t) \\ 
y(t)=Ch(t)+Dx(t)
\end{aligned}\]

<p>There are three continuous signals in the system: \(x(t), h(t), y(t)\),
which are:</p>

<ol>
  <li>
    <p>\(x(t)\in \mathbb{R}^p\) is the <code class="language-plaintext highlighter-rouge">input (or control) vector</code> that
describes the input continuous signal. 
Some papers would use \(u\) as the input vector, but we use \(x\) for better aligning with Mamba series.</p>
  </li>
  <li>
    <p>\(h(t)\in \mathbb{R}^n\) is the <code class="language-plaintext highlighter-rouge">state vector</code> that describes the system
states, and \(\dot{h}\) is the system dynamics <em>i.e.</em>
\(\frac{\partial h(t)}{\partial t}\).
Some papers would use \(x\) as the state vector, but we use \(h\) for better aligning with Mamba series.</p>
  </li>
  <li>
    <p>\(y(t) \in \mathbb{R}^q\) is the <code class="language-plaintext highlighter-rouge">output vector</code> that describes the
output continuous signal from the system.</p>
  </li>
</ol>

<p>There are four transition matrices in the system: \(A, B, C,\) and \(D\),
which are:</p>

<ol>
  <li>
    <p>\(A, \texttt{Dim}(A)=n\times n\) is the <code class="language-plaintext highlighter-rouge">state matrix</code> of the system that
describes the linear relations of the parameters in the system.</p>
  </li>
  <li>
    <p>\(B, \texttt{Dim}(B)=n\times p\) is the <code class="language-plaintext highlighter-rouge">input matrix</code> that describes how
input signals interact with the system.</p>
  </li>
  <li>
    <p>\(C, \texttt{Dim}(C)=q\times n\) is the <code class="language-plaintext highlighter-rouge">output matrix</code> that describes the
system output.</p>
  </li>
  <li>
    <p>\(D, \texttt{Dim}(D)=q\times p\) is the <code class="language-plaintext highlighter-rouge">feed-forward matrix</code> that
describes how input signals are directly involved in the output.</p>
  </li>
</ol>

<p><br /></p>

<h1 id="discretize-continuous-state-space-model">Discretize Continuous State Space Model</h1>

<p>The general formulation of an ODE is
\(\begin{aligned}
    \frac{d}{dt}h(t) = A h(t) + B x(t).
\end{aligned}\)</p>

<p>From the differential equation, we have \(\begin{aligned}
    h(t+\Delta t) - h(t) = \int_{s=t}^{s=t+\Delta t} A h(s) + B x(s)ds .
\end{aligned}\)</p>

<h2 id="bilinear-transform">Bilinear Transform</h2>

<p>By the Trapezoidal rule, we have</p>

\[\begin{aligned}
    h(t+\Delta t) - h(t) &amp;= \Delta t \frac{(Ah(t) + Bx(t) + Ah(t+\Delta t) + B x(t+\Delta t))}{2} \\
    &amp;= \frac{\Delta t}{2} Ah(t) + \frac{\Delta t}{2}Bx(t) + \frac{\Delta t}{2}Ah(t+\Delta t) + \frac{\Delta t}{2}Bx(t+\Delta t)
\end{aligned}\]

<p>By combining the terms, we have</p>

\[\begin{aligned}
    \textcolor{red}{h(t+\Delta t)} - \frac{\Delta t}{2}A\textcolor{red}{h(t+\Delta t)} &amp;= \textcolor{green}{h(t)} + \frac{\Delta t}{2} A \textcolor{green}{h(t)} + \textcolor{blue}{\frac{\Delta t}{2}B} x(t) + \textcolor{blue}{\frac{\Delta t}{2}B} x(t+\Delta t) \\
    ( \mathbf{I} - \frac{\Delta t}{2}A)h(t+\Delta t) &amp;= ( \mathbf{I}+ \frac{\Delta t}{2} A)h(t)  + \frac{\Delta t}{2} B(x(t) + x(t+\Delta t)) \\
    h(t+\Delta t) &amp;= ( \mathbf{I} - \frac{\Delta t}{2}A)^{-1} ( \mathbf{I}+ \frac{\Delta t}{2} A) h(t) + \frac{\Delta t}{2} ( \mathbf{I} - \frac{\Delta t}{2}A)^{-1} B (x(t) +  x(t+\Delta t))
\end{aligned}\]

<p>Since \(x\) is the constant and does no involve in the
integral, we have \(x(t) = x(t+\Delta t)\)</p>

\[\begin{aligned}
    h(t+\Delta t) &amp;= ( \mathbf{I} - \frac{\Delta t}{2}A)^{-1} ( \mathbf{I}+ \frac{\Delta t}{2} A) h(t) + \Delta t ( \mathbf{I} - \frac{\Delta t}{2} A)^{-1} B x(t+\Delta t)
\end{aligned}\]

<p>We can replace \(\frac{1}{2}\) with a variable \(\alpha\), so that we have
Generalized Bilinear Transformation (GBT).</p>

\[\begin{aligned}
    h(t+\Delta t) &amp;= \textcolor{red}{( \mathbf{I} - \Delta t \alpha A)^{-1} ( \mathbf{I}+ \Delta t \alpha A)} h(t) + \textcolor{green}{\Delta t ( \mathbf{I} - \Delta t \alpha A)^{-1} B} x(t+\Delta t)
\end{aligned}\]

<p>We can write the discretized state space model as</p>

\[\begin{aligned}
    h[t+1]  &amp;= \overline{A} h[t] + \overline{B} x[t+1] \\
    y[t+1]    &amp;= Ch[t+1]+Dx[t+1]
\end{aligned}\]

<p>where \(\overline{A}, \overline{B}\)  are discretized matrices</p>

\[\begin{aligned}
    \overline{A} &amp;= \textcolor{red}{(\mathbf{I} - \Delta t \alpha A)^{-1} ( \mathbf{I}+ \Delta t \alpha A)} \\
    \overline{B} &amp;= \textcolor{green}{\Delta t ( \mathbf{I} - \Delta t \alpha A)^{-1} B}
\end{aligned}\]

<p><br /></p>
<h2 id="zero-order-hold-transform-zoh">Zero-order Hold Transform (ZOH)</h2>

<h4 id="state-solution">State solution</h4>

<p>We derive the state solution from the continuous time-invariant dynamic equation  \(\frac{d}{dt}h(t) = A h(t) + Bx(t)\) 
and move \(A h(t)\) to the left-hand side</p>

\[\begin{aligned}
    \textcolor{red}{ \frac{d}{dt}h(t) - A h(t) } =  Bx(t) .
\end{aligned}\]

<p>Using the fact</p>

\[\begin{aligned}
    \textcolor{red}{\frac{d}{dt}[e^{-At}h(t)]} = e^{-At} \frac{d}{dt}h(t) - e^{-At} A h(t) = \textcolor{green}{e^{-At}} [ \textcolor{red}{\frac{d}{dt}h(t) - A h(t)} ]
\end{aligned}\]

<p>, we have</p>

\[\begin{aligned}
    \textcolor{red}{\frac{d}{dt}[e^{-At}h(t)]} = \textcolor{green}{e^{-At}} Bx(t) \\
\end{aligned}\]

<p>We integrate the both side from \(0\) to \(t\)</p>

\[\begin{aligned}
    \int_0^t \frac{d}{d \tau}[e^{-A\tau}h(\tau)] d\tau = \textcolor{red}{e^{-At}h(t) - h(0) = \int_0^t e^{-A\tau} Bx(\tau) d \tau} \\
\end{aligned}\]

<p>and move \(h(0)\) to the right-hand side</p>

\[\begin{aligned}
    e^{-At}h(t) = h(0) + \int_0^t e^{-A\tau} Bx(\tau) d \tau
\end{aligned}\]

<p>After multiplying \(e^{At}\) on both sides, we have the
state solution</p>

\[\label{state_solution}
\tag{eq:1}
\begin{aligned}
    h(t) = e^{At}h(0) + \int_0^t e^{A(t-\tau)} Bx(\tau) d \tau
\end{aligned}\]

<p>Note that \(\int_0^t e^{A(t-\tau)} Bx(\tau) d \tau\) is a
convolution, which can be done efficiently by multiplying the scalars in
the frequency domain. We will use this result to derive the Zero-order
Hold discretization.</p>

<p><br /></p>
<h4 id="zero-order-hold-discretization">Zero-order Hold Discretization</h4>

<p>We first define the relationship between discretized signals and
continuous signals.</p>

\[\begin{aligned}
    h[k] := h(k\Delta t), \quad \text{and} \quad x[k] := x(k\Delta t)
\end{aligned}\]

<p>where \(h[x]\) and \(c[k]\) is a zero-order transformed
signals from \(h(x)\) and \(x(x)\) with a sampling step \(k\).</p>

<p>Then using <a href="#eq:1">[eq:1]</a>, we have</p>

\[\begin{aligned}
    h[k+1] &amp;:= h(\textcolor{red}{(k+1)\Delta t}) \\
    &amp;= e^{A\textcolor{red}{(k+1)\Delta t}}h(0) + \int_0^{\textcolor{red}{(k+1)\Delta t}} e^{A(\textcolor{red}{(k+1)\Delta t}-\tau)} Bx(\tau) d \tau \\
    &amp;= e^{A(k+1)\Delta t}h(0) + \int_{\textcolor{green}{0}}^{\textcolor{green}{k\Delta t}} e^{A((k+1)\Delta t-\tau)} Bx(\tau) d \tau + \int_{\textcolor{green}{k\Delta t}}^{\textcolor{green}{(k+1)\Delta t}} e^{A((k+1)\Delta t-\tau)} Bx(\tau) d \tau \\
    &amp;= \textcolor{red}{e^{A\Delta t}}e^{Ak\Delta t}h(0) + \int_0^{k\Delta t} \textcolor{red}{e^{A\Delta t}} e^{A(k\Delta t-\tau)} Bx(\tau) d \tau + \int_{k\Delta t}^{(k+1)\Delta t} e^{A((k+1)\Delta t-\tau)} Bx(\tau) d \tau \\
    &amp;= e^{A\Delta t}\left( \textcolor{green}{ e^{Ak\Delta t}h(0) + \int_0^{k\Delta t} e^{A(k\Delta t-\tau)} Bx(\tau) d \tau} \right) + \int_{k\Delta t}^{(k+1)\Delta t} e^{A((k+1)\Delta t-\tau)} Bx(\tau) d \tau \\
    &amp;= e^{A\Delta t} \textcolor{green}{h(k\Delta t)} + \int_{k\Delta t}^{(k+1)\Delta t} e^{A((k+1)\Delta t-\tau)} Bx(\tau) d \tau \\
    &amp;= e^{A\Delta t} h[k] + e^{A((k+1)\Delta t} B \textcolor{red}{\int_{k\Delta t}^{(k+1)\Delta t} e^{-A\tau} x(\tau) d \tau} \\
    &amp;= e^{A\Delta t} h[k] + e^{A((k+1)\Delta t} B (\textcolor{red}{-A^{-1}e^{-A\tau}\Big|_{k\Delta t}^{(k+1)\Delta t}})x((k+1)\Delta t) \\
    &amp;= e^{A\Delta t} h[k] + e^{A((k+1)\Delta t} B (\textcolor{red}{-A^{-1}e^{-A(k+1)\Delta t} + -A^{-1}e^{-A k\Delta t}}) x((k+1)\Delta t) \\
    &amp;= e^{A\Delta t} h[k] + e^{A((k+1)\Delta t} B (\textcolor{red}{-A^{-1}})(e^{-A(k+1)\Delta t} + e^{-A k\Delta t}) x((k+1)\Delta t) \\
    &amp;= e^{A\Delta t} h[k] + \textcolor{green}{e^{A((k+1)\Delta t}} B (-A^{-1}) (\textcolor{green}{e^{-A(k+1)\Delta t} + e^{-A k\Delta t}}) x((k+1)\Delta t) \\
    &amp;= e^{A\Delta t} h[k] + BA^{-1} (\textcolor{green}{-\mathbf{I} + e^{A \Delta t}}) x[k+1] \\
    &amp;= \textcolor{red}{e^{A\Delta t}} h[k] + \textcolor{green}{\frac{\Delta t B}{\Delta t A} (e^{A \Delta t} - \mathbf{I})} x[k+1] \\
\end{aligned}\]

<p>Note that \(x\) is constant from \(k\Delta t\) to \((k+1)\Delta t\), and thereby \(x(k\Delta t) = x((k+1)\Delta t)\)</p>

<p>We can write the discretized state space model as</p>

\[\begin{aligned}
    h[t+1]  &amp;= \overline{A} h[t] + \overline{B} x[t+1] \\
    y[t+1]    &amp;= Ch[t+1]+Dx[t+1]
\end{aligned}\]

<p>where \(\overline{A}, \overline{B}\)  are discretized matrices</p>

\[\begin{aligned}
    \overline{A} &amp;= \textcolor{red}{e^{A\Delta t}} \\
    \overline{B} &amp;= \textcolor{green}{ {(A \Delta t)}^{-1} (e^{A \Delta t} - \mathbf{I}) \Delta t B}
\end{aligned}\]

<p>We can perform an Euler approximation to simplify \(B\).
The first-order Taylor expansion of \(e^{A \Delta t}\) around zero is given by:</p>

\[\begin{aligned}
e^{A \Delta t} \approx I + A \Delta t
\end{aligned}\]

<p>Therefore, we have</p>

\[\begin{aligned}
    \overline{B} &amp;= {(A \Delta t)}^{-1} (\textcolor{red}{e^{A \Delta t}} - \mathbf{I}) \Delta t B
    \approx {(A \Delta t)}^{-1} (\textcolor{red}{I + A \Delta t} - \mathbf{I}) \Delta t B =  \Delta t B
\end{aligned}\]

<h1 id="discrete-time-state-space-model">Discrete-time State Space Model</h1>

<p>We can write the continuous-time state space model</p>

\[\begin{aligned}
\dot{x}(t)=Ax(t)+Bu(t) \\ 
y(t)=Cx(t)+Du(t)
\end{aligned}\]

<p>as a discrete-time state space model</p>

\[\begin{aligned}
    x[t]  &amp;= \overline{A} x[t-1] + \overline{B} u[t] \\
    y[t]  &amp;= C x[t] + D u[t] \\
\end{aligned}\]

<p>by using the GBL or ZOH</p>

\[\begin{aligned}[c]
&amp;\text{Bilinear:}\\
\overline{A}&amp;= (\mathbf{I} - \Delta t \alpha A)^{-1} ( \mathbf{I}+ \Delta t \alpha A)\\
\overline{B} &amp;= \Delta t ( \mathbf{I} - \Delta t \alpha A)^{-1} B
\end{aligned}
\quad\quad
\begin{aligned}[c]
&amp;\text{ZOH:}\\
\overline{A} &amp;= e^{A\Delta t}\\
\overline{B} &amp;= {(\Delta t A)}^{-1} (e^{A \Delta t} - \mathbf{I}) \Delta t B \approx \Delta t B\\
\end{aligned}\]

<p><br /></p>
<h2 id="convolution-form">Convolution Form</h2>

<p>Letting \(h_{-1} = 0\) and unrolling the \(y_k\)</p>

\[\begin{aligned}[c]
&amp;\text{t=0}\\
h_0 &amp;= \overline{B}x_0\\
y_0 &amp;= C\overline{B}x_0 + Dx_0\\
\end{aligned}
\quad
\begin{aligned}[c]
&amp;\text{t=1}\\
h_1 &amp;= \overline{A}\overline{B}x_0 + \overline{B}x_1\\
y_1 &amp;= C\overline{A}\overline{B}x_0 + C\overline{B}x_1 + Dx_1\\
\end{aligned}
\quad
\begin{aligned}[c]
&amp;\text{t=2}\\
h_2 &amp;= \overline{A}^2\overline{B}x_0 + \overline{A}\overline{B}x_1 + \overline{B}x_2\\
y_2 &amp;= C\overline{A}^2\overline{B}x_0 + C\overline{A}\overline{B}x_1 + C\overline{B}x_2 + Dx_2\\
\end{aligned}\]

<p>Therefore, when \(t=k\)</p>

\[\begin{aligned}
h_k &amp;= \overline{A}^k\overline{B}x_0 +...+ \overline{A}\overline{B}x_{k-1} + \overline{B}x_k\\
y_k &amp;= C\overline{A}^k\overline{B}x_0 + C\overline{A}^{k-1}\overline{B}x_1 +...+ C\overline{A}\overline{B}x_{k-1} + C\overline{B}x_k + Dx_k \\
\end{aligned}\]

<p>In other words, \(y_t\) is a single (non-circular) convolution</p>

\[\begin{aligned}
y_t = \sum_{\tau=0}^k \overline{C}\overline{A}^{\tau}\overline{B} x_{t-\tau}  + Dx_t
\end{aligned}\]

<p>which can be computed very efficiently with FFTs,
provided that \(\overline{K}\) is known.</p>

<p>\(\begin{aligned}
    y_t = \overline{K} \ast x + Dx_t
\end{aligned}\)
where 
\(\begin{aligned}
    \overline{K} \in \mathbb{R}^L := \kappa_L(\overline{A}, \overline{B}, \overline{C}) := (\overline{C}\overline{A}^i\overline{B})_{i \in [L]} = (\overline{C}\overline{B}, \overline{C}\overline{A}\overline{B}, ..., , \overline{C}\overline{A}^{L-1}\overline{B})
\end{aligned}\)</p>

<p><br /></p>
<h2 id="parallel-associative-scan">Parallel Associative Scan</h2>

<p>Consider an example for illustrating a parallel associative scan (or
parallel prefix sum/scan): \(\begin{aligned}
    h_t  &amp;= \overline{A} h_{t-1} + \overline{B} x_t \\
\end{aligned}\)</p>

<p>We unroll the recursive equation with \(h_0 = 0\).</p>

\[\begin{aligned}
    h_1  &amp;= \overline{B} x_1 \\
    h_2  &amp;= \overline{A} h_1 + \overline{B} x_2 = \overline{A}\overline{B} x_1 + \overline{B} x_2\\
    h_3  &amp;= \overline{A} h_2 + \overline{B} x_3 = \overline{A}(\overline{A}\overline{B} x_1 + \overline{B} x_2) + \overline{B} x_3 = \overline{A}^2\overline{B} x_1 + \overline{A}\overline{B} x_2 + \overline{B} x_3\\
    h_4  &amp;= \overline{A} h_3 + \overline{B} x_4 = \overline{A}(\overline{A}^2\overline{B} x_1 + \overline{A}\overline{B} x_2 + \overline{B} x_3) + \overline{B} x_4 = \overline{A}^3\overline{B} x_1 + \overline{A}^2\overline{B} x_2 + \overline{A}\overline{B} x_3 + \overline{B} x_4\\
\end{aligned}\]

<p>Define an operand \(e_i = (\overline{A}, \overline{B}x_i)\) and operator
\(e_i \cdot e_j\) for parallel associative scan such that</p>

\[\begin{aligned}
e_1 \cdot e_2 &amp;= (\overline{A}, \overline{B}x_1) \cdot (\overline{A}, \overline{B}x_2) = (\overline{A}^2, \overline{A}\overline{B}x_1 + \overline{B}x_2) = (\overline{A}^2, h_2) \\
e_1 \cdot e_2 \cdot e_3 &amp;= (\overline{A}^2, \overline{A}\overline{B}x_1 + \overline{B}x_2) \cdot (\overline{A}, \overline{B}x_3) = (\overline{A}^3, \overline{A}^2\overline{B} x_1 + \overline{A}\overline{B} x_2 + \overline{B} x_3) = (\overline{A}^2, h_3) \\
\end{aligned}\]

<p>Therefore, we can parallelize the computation with a
parallel associative scan into a tree structure, for example, \(h_4\) can
be computed by</p>

\[\begin{aligned}
(e_1 \cdot e_2 \cdot e_3 \cdot e_4) &amp;= (e_1 \cdot e_2) \cdot (e_3 \cdot e_4) \\
&amp;=  (\overline{A}^2, \overline{A}\overline{B}x_1 + \overline{B}x_2) \cdot  (\overline{A}^2, \overline{A}\overline{B}x_3 + \overline{B}x_4) \\
&amp; =  (\overline{A}^4, \overline{A}^3\overline{B} x_1 + \overline{A}^2\overline{B} x_2 + \overline{A}\overline{B} x_3 + \overline{B} x_4) \\
&amp; =  (\overline{A}^4, h_4) \\
\end{aligned}\]

<h1 id="references">References</h1>

<p><a id="1">[1]</a> 
A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré, “Hippo: Recurrent memory with optimal
polynomial projections,” Advances in neural information processing systems, vol. 33, pp. 1474–
1487, 2020.</p>

<p><a id="2">[2]</a> 
A. Gu, K. Goel, A. Gupta, and C. Ré, “On the parameterization and initialization of diagonal state
space models,” Advances in Neural Information Processing Systems, vol. 35, pp. 35971–35983,
2022.</p>

<p><a id="3">[3]</a> 
A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv
preprint arXiv:2312.00752, 2023.</p>

<p><a id="4">[4]</a> 
<a href="https://en.wikipedia.org/wiki/State-space_representation">https://en.wikipedia.org/wiki/State-space_representation</a></p>]]></content><author><name></name></author><category term="ml" /><category term="paper" /><category term="algorithm" /><category term="SSM" /><summary type="html"><![CDATA[State Space Models]]></summary></entry><entry><title type="html">Install/Update CUDA on Ubuntu Machines</title><link href="/blog/2024/install_cuda/" rel="alternate" type="text/html" title="Install/Update CUDA on Ubuntu Machines" /><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>/blog/2024/install_cuda</id><content type="html" xml:base="/blog/2024/install_cuda/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>This document contain the step to install/update CUDA on Ubuntu machines</p>

<h1 id="installation">Installation</h1>

<h4 id="step-1-download-cuda-from-nvidia-official-website-either-deb-local-deb-network-or-runfile-local-would-work">Step 1. Download cuda from NVIDIA official <a href="https://developer.nvidia.com/cuda-downloads">website</a>. Either Deb (local), deb (network), or runfile (local) would work.</h4>

<p><br /></p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/install_cuda/cuda-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/install_cuda/cuda-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/install_cuda/cuda-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/install_cuda/cuda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="step-2-follow-the-instructions-on-the-nvidia-official-website-to-download-cuda">Step 2. Follow the instructions on the NVIDIA official website to download CUDA.</h4>

<p><br /></p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/install_cuda/install_cuda-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/install_cuda/install_cuda-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/install_cuda/install_cuda-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/install_cuda/install_cuda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="step-3-reboot-the-machine">Step 3. Reboot the machine</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>reboot
</code></pre></div></div>
<p><br /></p>

<h4 id="step-4-check-system-cuda-after-reboot">Step 4. Check system cuda after reboot</h4>
<p><br /></p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/install_cuda/check_cuda-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/install_cuda/check_cuda-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/install_cuda/check_cuda-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/install_cuda/check_cuda.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="step-5-setup-the-environment-in-bashrc-please-see-the-official-document">Step 5. Setup the environment in ~/.bashrc. Please see the official <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup">document</a>.</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">### add cuda path in ~/.bashrc</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span>/usr/local/cuda/bin<span class="k">${</span><span class="nv">PATH</span>:+:<span class="k">${</span><span class="nv">PATH</span><span class="k">}}</span>
<span class="nb">export </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/local/cuda/lib64<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span>:+:<span class="k">${</span><span class="nv">LD_LIBRARY_PATH</span><span class="k">}}</span>
</code></pre></div></div>
<p><br /></p>

<h4 id="step-6-source-new-environment">Step 6. Source new environment</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">source</span> ~/.bashrc
</code></pre></div></div>
<p><br /></p>

<h4 id="step-7-test-driver">Step 7. Test driver</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvidia-smi
</code></pre></div></div>
<p><br /></p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/install_cuda/test_driver-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/install_cuda/test_driver-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/install_cuda/test_driver-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/install_cuda/test_driver.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h4 id="step-8-test-cuda-compiler">Step 8. Test cuda compiler</h4>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>nvcc <span class="nt">-V</span>
</code></pre></div></div>
<p><br /></p>
<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/install_cuda/test_cudnn-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/install_cuda/test_cudnn-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/install_cuda/test_cudnn-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/posts/install_cuda/test_cudnn.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<p><br /></p>

<h1 id="reference">Reference:</h1>
<ul>
  <li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li>
  <li><a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a></li>
  <li><a href="https://developer.nvidia.com/cuda-toolkit-archive">https://developer.nvidia.com/cuda-toolkit-archive</a></li>
  <li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#environment-setup</a></li>
</ul>]]></content><author><name></name></author><category term="linux" /><category term="ubuntu" /><category term="nvidia" /><category term="commands" /><summary type="html"><![CDATA[Install/Update CUDA on Ubuntu Machines]]></summary></entry></feed>