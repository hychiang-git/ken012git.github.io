<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>HiPPO Matrices | Hung-Yueh Chiang</title> <meta name="author" content="Hung-Yueh Chiang"> <meta name="description" content="HiPPO Matrices"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ut_shield.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="/blog/2024/hippo_matrices/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <img width="60px" src="/assets/img/ut_longhorns.webp" class="navbar-brand logo" alt="blank logo"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hung-Yueh </span>Chiang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">HiPPO Matrices</h1> <p class="post-meta">July 11, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fas fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fas fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/paper"> <i class="fas fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/algorithm"> <i class="fas fa-hashtag fa-sm"></i> algorithm</a>   <a href="/blog/tag/ssm"> <i class="fas fa-hashtag fa-sm"></i> SSM</a>   </p> </header> <article class="post-content"> <style>#tableOfContents{font-size:1.6em}</style> <details style="background-color: #F5F5F5;"> <summary id="tableOfContents">Table of Contents</summary> <ul style="font-size:1.4em"> <li><a href="#introduction">Introduction</a></li> <li><a href="#derive-hippo-matrices">Derive HiPPO Matrices</a></li> <ul> <li><a href="#backgrouds">Backgrounds</a></li> <ul> <li><a href="#approximation">Approximation</a></li> <li><a href="#orthogonal-polynomial-basis">Orthogonal Polynomial Basis</a></li> <li><a href="#tilted-measure-and-basis">Tilted Measure and Basis</a></li> </ul> <li><a href="#the-projection-and-coefficients">The Projection and Coefficients</a></li> <ul> <li><a href="#approximate-input-function">Approximate Input Function</a></li> <li><a href="#reconstruct-input-function">Reconstruct Input Function</a></li> <li><a href="#coefficient-dynamics">Coefficient Dynamics</a></li> </ul> <li><a href="#case-hippo-legs">Case: HiPPO-LegS</a></li> <ul> <li><a href="#properties-of-legendre-polynomials">Properties of Legendre Polynomials</a></li> <li><a href="#shifted-and-scaled-legendre-polynomials">Shifted and Scaled Legendre Polynomials</a></li> <li><a href="#derivatives-of-legendre-polynomials">Derivatives of Legendre Polynomials</a></li> <li><a href="#derive-hippo-legs">Derive HiPPO-LegS</a></li> </ul> </ul> <li><a href="#references">References</a></li> </ul> </details> <p><br></p> <h1 id="introduction">Introduction</h1> <p>This document provides the mathematic derivations of the HiPPO matrices that help readers understand the formulas. The contents are extracted and summarized from the original papers cited in <a href="#references">references</a>.</p> <p><br></p> <h1 id="derive-hippo-matrices">Derive HiPPO Matrices</h1> <p>The contents are extracted and summarized from the original paper <a href="#1">[1]</a>, but we show more details in the derivations to help people to understand.</p> <p><br></p> <h2 id="backgrounds">Backgrounds</h2> <p>HiPPO considers a linear time-invariant (LTI) ODEs such that</p> \[\Large \begin{aligned} \dot{c}(t)=Ac(t)+Bf(t) . \end{aligned}\] <ol> <li> <p>\(c(t)\) is the coefficient vector that describes the combination of the basis functions, which corresponds to the state vector \(h(t)\) in the continuous state space model.</p> </li> <li> <p>\(f(t)\) is the input signal that corresponds to the input vector \(x(t)\) in the continuous state space model.</p> </li> <li> <p>\(\dot{c}(t)\) is the system dynamics that represents a compression of the history of \(f\) that satisfies linear dynamics.</p> </li> </ol> <p><br></p> <h4 id="approximation">Approximation</h4> <p>Given a time-varying measure family \(u^{(t)}\) supported on \(( -\inf, t]\), a sequence of basis functions \(\mathcal{G} = \text{span} \{ {g_n}^{(t)} \}_{n \in [N]}\)] , and a continuous function \(f: \mathbb{R}_{\geq0} \mapsto \mathbb{R}\), HiPPO defines an operator that maps \(f\) to the optimal projection coefficients \(c: \mathbb{R}_{\geq0}\mapsto \mathbb{R}^N\), such that</p> \[\Large \begin{aligned} g^{(t)} = \text{argmin}_{g \in \mathcal{G}}\left\lVert f_{x \leq t} - g\right\rVert_{\mu^{(t)}} \end{aligned} \quad \text{and} \quad \Large \begin{aligned} g^{(t)} = \sum^{N-1}_{n=0} c_n(t) g_n^{(t)} \end{aligned}\] <p><br></p> <h4 id="measures">Measures</h4> <p>At every \(t\), the approximation quality is defined with respect to a measure \(\mu^{(t)}\) supported on \((-\infty, t]\). We seek some polynomial \(g^{(t)}\) of degree at most \(N-1\) that minimizes the error \(\begin{aligned} \left\lVert f_{x \leq t} - g^{(t)}\right\rVert_{L_2(\mu^{(t)})} \end{aligned}\). Suppose the measures \(\mu^{(t)}\) are sufficiently smooth across their domain as well as in time and have desities \(\begin{aligned} \omega(t, x) = \frac{d\mu^{(t)}}{d\lambda}(x) \end{aligned}\) with respect to the Lebesgue measure \(d\lambda(x)=dx\) such that \(\omega\) is \(C^1\) almost everywhere. Therefore, we have \(\begin{aligned} \omega(t, x) = \frac{d\mu^{(t)}(x)}{dx} \end{aligned}\). Thus, integrating against \(d\mu^{(t)}(x)\) can be rewritten as integrating against \(\omega(t, x)dx\). That is \(\begin{aligned} \int d\mu^{(t)}(x) = \int \omega(t, x)dx \end{aligned}\).</p> <p><br></p> <h4 id="orthogonal-polynomial-basis">Orthogonal Polynomial Basis</h4> <p>We can use a sequence of orthogonal polynomials (OPs) as our basis to approximate the continuous function \(f\). A sequence of OPs \(P_0(x), P_1(x), ...\) satisfying:</p> <ol> <li> <p>\(\text{deg}(P_i)=i\), and</p> </li> <li> <p>\(\langle P_i, P_j \rangle = \int P_i(x) P_j(x) d\mu(x) = 0\) for all \(i \neq j\)</p> </li> </ol> <p>A sequence of OPs \(g = \{P_n\}_{n \in \mathbb{N}}\) of degree \(\text{deg}(g)&lt;N\) that approximate a function \(f\) is then given by</p> \[\begin{aligned} &amp; \sum_{i=0}^{N-1} c_i \frac{P_i(x)}{\left\lVert P_i\right\rVert^2_\mu} \quad \text{where } c_i = \langle f, P_i \rangle = \int f(x)P_i(x)d\mu(x) \\ \end{aligned}.\] <p>This projects an input sequence signal onto a sequence of OPs, and \(c_i\) describes the magnitude of the \(P_i\) to reconstruct the input signal.</p> <p>Let \(\{P_n\}_{n \in \mathbb{N}}\) denote a sequence of OPs with respect to some base measure \(\mu\). Similarly, define \(\{ {P_n}^{(t)} \}_{n \in \mathbb{N}}\) to be a sequence of OPs with respect to the time-varying measure \(\mu^{(t)}\). Let \({p_n}^{(t)}\) be the normalized version of \({P_n}^{(t)}\) (<em>i.e.,</em> have norm 1), and define \(\begin{aligned} p_n(t, x) = {p_n}^{(t)}(x) \end{aligned}\). The \({P_n}^{(t)}\) are not required to be normalized, while the \({p_n}^{(t)}\) are.</p> <p><br></p> <h4 id="tilted-measure-and-basis">Tilted Measure and Basis</h4> <p>The goal is simply to store a compressed representation of functions, which can use any basis, not necessarily OPs. For any scaling function \(\begin{aligned} \chi(t, x) = \chi^{(t)}(x) \end{aligned}\) such that \(p_n(x)\chi(x)\) are orthogonal with respect to the density \(\omega/\chi^2\) at every time \(t\). Thus, we can choose this alternative basis and measure to perform the projections.</p> <p>Define \(\nu\) to be the <code class="language-plaintext highlighter-rouge">normalized measure</code> with density proportional to \(\omega^{t}/(\chi^{(t)})^2\). We will calculate the normalized measure and the orthonormal basis for it.</p> <p>Let \(\zeta(t)\) to be the normalization constant: \(\begin{aligned} \zeta(t) = \int \frac{\omega}{\chi^2} = \int \frac{\omega^{(t)}(x)}{({\chi^{(t)}(x)})^2}dx \end{aligned}\) , so that \(\nu^{(t)}\) has density \(\frac{\omega^{(t)}(x)}{\zeta(t)(\chi^{(t)}(x))^2}\) If \(\chi(t, x) = 1\) (no tilting), this constant is \(\chi(t) = 1\).</p> <p>Note that (dropping the dependence on x inside the integral for shorthand)</p> \[\begin{aligned} \left\lVert \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)}\right\rVert_{\nu^{(t)}}^2 &amp;= \int {( \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)})}^2 \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\ &amp;= \int (p_n^{(t)})^2 \omega^{(t)} \\ &amp;= \left\lVert p_n^{(t)}\right\rVert_{\mu^{(t)}}^2 = 1 \quad. \end{aligned}\] <p>Thus we define the <code class="language-plaintext highlighter-rouge">orthogonal basis</code> for \(\nu^{(t)}\) (normalized measurement)</p> \[\Large \label{orthogonal_basis} \tag{eq:1} \begin{aligned} {g_n}^{(t)} = \lambda_n \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)}, \quad n \in \mathbb{N} \end{aligned}\] <p>Therefore, we have</p> \[\Large \label{gn_gm} \tag{eq:2} \begin{aligned} \langle {g_n}^{(t)}, {g_m}^{(t)} \rangle_{\nu^{(t)}} = \lambda_n^2 \delta_{n,m} \quad. \end{aligned}\] <p>Note that when \(\lambda_n=\pm1\), the basis \(\{ {g_n}^{(t)} \}\) is an orthonormal basis with respect to the measure \(\nu^{(t)}\), at every time \(t\). Notationally, let \({g_n}^{(t)}(x)=g_n(t, x)\) as usual.</p> <p><br></p> <h2 id="the-projection-and-coefficients">The Projection and Coefficients</h2> <p>Given a choice of measures \(\nu\) and basis functions \(g\), we compute the coefficients \(c(t)\) to approximate a \(C^1\)-smooth function which is seen <em>online</em> \(f(x)_{x \leq t}\).</p> <p><br></p> <h4 id="approximate-input-function">Approximate Input Function</h4> <p>We project the input function \(f\) to basis functions \(g\), and \(c(t)\) is the coefficient vector that describes the magnitude of the basis functions. Using the result from the formula <a href="#eq:1">[eq:1]</a>, we have</p> \[\label{approx_f} \tag{eq:3} \begin{aligned} c_n(t) &amp;= \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} \\ &amp;= \int f \textcolor{red}{ {g_n}^{(t)} } \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\ &amp;= \int f \textcolor{red} { \lambda_n \zeta(t)^{\frac{1}{2}} {p_n}^{(t)} \chi^{(t)} } \frac{\omega^{(t)}}{\zeta(t)(\chi^{(t)})^2} \\ &amp;= \int f \lambda_n \zeta(t)^{-\frac{1}{2}} {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}} \\ &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}} \end{aligned}\] <p><br></p> <h4 id="reconstruct-input-function">Reconstruct Input Function</h4> <p>The function \(f\) can be approximated by storing its coefficients with respect to the basis. At any time \(t\), \(f(x)_{x \leq t}\) can be explicitly reconstructed by using \(c_n(t) = \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}}\) and the formula <a href="#eq:2">[eq:2]</a>, such that</p> \[\label{reconstruct} \tag{eq:4} \begin{aligned} f(x)_{x \leq t} \approx {g}^{(t)} &amp;= \sum_{n=0}^{N-1} \textcolor{red} { \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} } \frac{ {g_n}^{(t)} }{ {\left \lVert {g_n}^{(t)} \right \rVert }_{\nu^{(t)}}^2} \\ &amp;= \sum_{n=0}^{N-1} \textcolor{red}{c_n(t)} \frac{ {g_n}^{(t)} }{ \textcolor{green}{ {\left \lVert {g_n}^{(t)} \right \rVert }_{\nu^{(t)}}^2} } \\ &amp;= \sum_{n=0}^{N-1} c_n(t) \frac{ {g_n}^{(t)} }{ \textcolor{green}{ \langle {g_n}^{(t)}, {g_n}^{(t)} \rangle_{\nu^{(t)}} } } \\ &amp;= \sum_{n=0}^{N-1} c_n(t) \frac{ {g_n}^{(t)} }{ \textcolor{green}{\lambda_n^2} } \\ &amp;= \sum_{n=0}^{N-1} \lambda_n^{-2} c_n(t) {g_n}^{(t)} \\ &amp;= \sum_{n=0}^{N-1} \lambda_n^{-1} \zeta^{\frac{1}{2}} c_n(t) {p_n}^{(t)} \chi^{(t)} \\ \end{aligned}\] <p><br></p> <h4 id="coefficient-dynamics">Coefficient Dynamics</h4> <p>The coefficients \(c(t)\) encode information about the history of \(f\) and allow online predictions. We compute these coefficients over time by viewing them as a dynamical system. Differentiating \(c_n(t)\), we have</p> \[\begin{aligned} \frac{d}{dt} c_n(t) &amp;= \frac{d}{dt}( \langle f(x)_{x \leq t}, {g_n}^{(t)} \rangle_{\nu^{(t)}} ) \\ &amp;= \frac{d}{dt}( \zeta(t)^{-\frac{1}{2}} \lambda_n \int f \textcolor{red}{ {p_n}^{(t)} } \textcolor{green}{ \frac{\omega^{(t)}}{\chi^{(t)}} } ) \\ &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f \textcolor{red}{ \frac{\partial}{\partial t}({p_n}^{(t)}) } \frac{\omega^{(t)}}{\chi^{(t)}} + \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)} \textcolor{green}{ \frac{\partial}{\partial t}(\frac{\omega^{(t)}}{\chi^{(t)}} ) } \\ &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f(x) (\frac{\partial}{\partial t} p_n(t, x)) \frac{\omega}{\chi}(t, x) dx + \int f(x) (\zeta^{-\frac{1}{2}}\lambda_n p_n(t, x)) (\frac{\partial}{\partial t} \frac{\omega}{\chi}(t, x)) dx \quad. \end{aligned}\] <p><br></p> <h2 id="case-hippo-legs">Case: HiPPO-LegS</h2> <p><br></p> <h4 id="properties-of-legendre-polynomials">Properties of Legendre Polynomials</h4> <p>An especially compact expression for the Legendre polynomials is given by Rodrigues’ formula <a href="#4">[4]</a>:</p> \[\begin{aligned} P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n}(x^2 - 1)^n . \end{aligned}\] <p>The first few Legendre polynomials are:</p> \[\begin{aligned} P_0(x) &amp;= 1 \\ P_1(x) &amp;= x \\ P_2(x) &amp;= \frac{1}{2} (3x^2 - 1) \\ P_3(x) &amp;= \frac{1}{2} (5x^3 - 3x) \\ \end{aligned}\] <p>The Legendre polynomials are orthogonal over \((-1,1)\) with respect to the measure \(\omega^{leg}=\boldsymbol{1}_{[-1, 1]}\) and they satisfy</p> \[\label{legendre} \tag{eq:5} \begin{aligned} \frac{2n+1}{2}\int_{-1}^{1} P_n(x) P_m(x) dx = \delta_{mn} \end{aligned}\] <p>where \(\delta_{mn}\) is Kronecker delta, equal to \(1\) if \(m = n\) and to \(0\) otherwise, such that</p> \[\begin{aligned} \delta_{mn} = \begin{cases} 0,&amp; \text{if } m \neq n \\ 1,&amp; \text{if } m = n \\ \end{cases} \end{aligned}\] <p>Also, they satisfy</p> \[\begin{aligned} P_n(1) = 1 \\ P_n(-1) = (-1)^n \\ \end{aligned}\] <p>so called "Pointwise evaluations".</p> <p><br></p> <h4 id="shifted-and-scaled-legendre-polynomials">Shifted and Scaled Legendre Polynomials</h4> <p>The "shifted" Legendre polynomials are a set of functions analogous to the Legendre polynomials, but defined on the interval \((0, 1)\) <a href="#5">[5]</a>. They obey the orthogonality relationship</p> \[\begin{aligned} (2n+1)\int_{0}^{1} P_n(x) P_m(x) dx = \delta_{mn} \end{aligned}\] <p>We will also consider scaling the Legendre polynomials to be orthogonal on the interval \([0, t]\). Let \(u=\frac{2x}{t}-1\), and apply a change of variables from <a href="#eq:5">[eq:5]</a></p> \[\begin{aligned} &amp; \frac{2n+1}{2}\int_{u=-1}^{u=1} P_n(u) P_m(u) du \\ &amp; \text{since } u = \frac{2x}{t}-1 \text{ , we have } du = \frac{2}{t}dx \\ &amp; \text{since } x = \frac{t}{2}(u+1) \text{ , we have } x = 0 \text{ when } u=-1 \text{ ,and } x=t \text{ when } u=1 \\ &amp; \text{replace } u \text{ with } x \\ &amp;= \frac{2n+1}{2}\int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{2}{t}dx \\ &amp;= (2n+1)\int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{t}dx \\ &amp;= (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{2}\omega^{leg}(\frac{2x}{t}-1) \frac{1}{t} dx \\ &amp;= \delta_{nm} \end{aligned}\] <p>Therefore, with respect to the measure \(\omega_t=\boldsymbol{1}_{[0, t]} / t\) (which is a probability measure for all t), we have</p> \[\begin{aligned} &amp; (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \frac{1}{2}\omega^{leg}(\frac{2x}{t}-1) \frac{1}{t} dx \\ &amp;= (2n+1) \int_{x=0}^{x=t} P_n(\frac{2x}{t}-1) P_m(\frac{2x}{t}-1) \omega_{t}(\frac{2x}{t}-1) dx \\ \end{aligned}\] <p>Then, the normalized orthogonal polynomials in the interval \([0, t]\) with starting point is \(\textcolor{red}{0}\) and step size \(\textcolor{green}{t}\) are</p> \[\begin{aligned} (2n+1)^{\frac{1}{2}} P_n(\frac{2x}{t}-1). \quad \left( =(2n+1)^{\frac{1}{2}} P_n(\frac{2(x-\textcolor{red}{0})}{\textcolor{green}{t}}-1) \right) \end{aligned}\] <p>Generalize to any interval \([t-\theta, t]\), we use \(\textcolor{red}{t-\theta}\) as the starting point and \(\textcolor{green}{\theta}\) as the step size. After replacing the notation, we have</p> \[\begin{aligned} &amp;(2n+1)^{\frac{1}{2}} P_n(\frac{2(x - \textcolor{red}{ (t-\theta)}) }{\textcolor{green}{\theta}}-1) \\ &amp;=(2n+1)^{\frac{1}{2}} P_n(\frac{2x-2t+2\theta}{\theta} - 1) \\ &amp;=(2n+1)^{\frac{1}{2}} P_n(\frac{2(x-t)}{\theta} + 1) \\ \end{aligned}\] <p>, which is orthonormal for the uniform measure \(\omega_\theta= \frac{1}{ \theta}\boldsymbol{1}_{[t-\theta, t]}\)</p> <p><br></p> <h4 id="derivatives-of-legendre-polynomials">Derivatives of Legendre Polynomials</h4> <p>use the following recurrence relations (the integration of Legendre polynomial <a href="#4">[4]</a>)</p> \[\label{legendre_recur_1} \tag{eq:6} \begin{aligned} (2n+1)P_n &amp;= P'_{n+1} - P'_{n-1} \end{aligned}\] <p>and</p> \[\label{legendre_recur_2} \tag{eq:7} \begin{aligned} P'_{n+1} &amp;= (n+1)P_n + xP'_n \end{aligned}\] <p>The first equation <a href="#eq:6">[eq:6]</a> yields</p> \[\begin{aligned}[c] P'_{n+1} &amp;= (2\mathbf{n}+1)P_\mathbf{n} + \textcolor{red}{P'_{n-1}} \\ \textcolor{red}{P'_{n-1}} &amp;= (2(\mathbf{n-2})+1)P_\mathbf{n-2} + \textcolor{green}{P'_{n-3}} \\ \textcolor{green}{P'_{n-3}} &amp;= (2(\mathbf{n-4})+1)P_\mathbf{n-4} + \textcolor{blue}{P'_{n-5}} \\ ... \end{aligned} \quad\quad \begin{aligned}[c] P'_{n} &amp;= (2(\mathbf{n-1})+1)P_\mathbf{n-1} + \textcolor{red}{P'_{n-2}} \\ \textcolor{red}{P'_{n-2}} &amp;= (2(\mathbf{n-3})+1)P_\mathbf{n-3} + \textcolor{green}{P'_{n-4}} \\ \textcolor{green}{P'_{n-4}} &amp;= (2(\mathbf{n-5})+1)P_\mathbf{n-5} + \textcolor{blue}{P'_{n-6}} \\ ... \end{aligned}\] <p>Therefore we have</p> \[\begin{aligned} &amp; P'_{n+1} = (2n+1)P_n + (2n-3)P_{n-2} + (2n-7)P_{n-4} ... \quad \text{ and} \\ &amp; P'_n = (2n-1)P_{n-1} + (2n-5)P_{n-3} + (2n-9)P_{n-5} ... \end{aligned}\] <p>where the sum stops at \(P_0\) or \(P_1\). By summing \(P'_{n+1} + P'_n\), we have</p> \[\begin{aligned} &amp; P'_{n+1} + P'_n = \textcolor{red}{(2n+1)P_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\ &amp; P'_{n+1} + P'_n = \textcolor{red}{nP_n + (n+1)P_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \text{ (using eq:7)}\\ &amp; P'_{n+1} + P'_n = nP_n + \textcolor{green}{P'_{n+1} - xP'_n} + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\ &amp; P'_n \textcolor{green}{+ xP'_n} = nP_n + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\ \end{aligned}\] <p>Therefore, we have</p> \[\label{legendre_result} \tag{eq:8} \begin{aligned} (x+1)P'_n(x) &amp;= nP_n + (2n-1)P_{n-1}+ (2n-3)P_{n-2} + (2n-5)P_{n-3} + (2n-7)P_{n-4} ... \\ \end{aligned}\] <p>We will use the result <a href="#eq:8">[eq:8]</a> to derive HiPPO-LegS.</p> <p><br></p> <h4 id="derive-hippo-legs">Derive HiPPO-LegS</h4> <p>Now we have the measure \(\omega(t, x) = \frac{1}{t}\mathbb{1}_{[0,t]}\) and the basis</p> \[g_n(t, x) = p_n(t, x) = (2n+1)^{\frac{1}{2}}P_n(\frac{2x}{t}-1)\] <p>where \(P_n\) are the basic Legendre polynomials.</p> <p>Since the basis functions \(g_n(t, x)\) are orthonormal with respect to the measure, we have the tiling function \(\chi(t, x) = 1\) (no tilting), \(\zeta(t, x) = 1\), \(\lambda_n=1\)</p> <p>Plug \(\chi, \zeta, \lambda_n\) into <a href="#eq:3">[eq:3]</a>, we have</p> \[\begin{aligned} c_n(t) &amp;= \zeta(t)^{-\frac{1}{2}} \lambda_n \int f {p_n}^{(t)}\frac{\omega^{(t)}}{\chi^{(t)}} = \int f {p_n}^{(t)} \omega^{(t)} \end{aligned}\] <ul> <li><strong>HiPPO-LegS Derivatives</strong></li> </ul> <p>We first differentiate the measure and basis:</p> \[\begin{aligned} \frac{\partial}{\partial t} \omega(t, \cdot) &amp;= -t^{-2} \mathbb{1}_{[0, t]} + t^{-1} \delta_t = t^{-1} (-\omega(t) + \delta_t) \\ \frac{\partial}{\partial t} g_n(t, x) &amp;= -(2n+1)^{\frac{1}{2}}2xt^{-2} {P'}_n(\frac{2x}{t}-1) \\ &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} (\textcolor{red}{\frac{2x}{t}-1}+1) {P'}_n(\textcolor{red}{\frac{2x}{t}-1}) \quad. \end{aligned}\] <p>Now define \(z = \frac{2x}{t} - 1\) and apply the result of derivatives of Legendre polynomials in the equation <a href="#eq:8">[eq:8]</a>.</p> \[\begin{aligned} \frac{\partial}{\partial t} g_n(t, x) &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} (z+1) {P'}_n(z) \\ &amp;= -(2n+1)^{\frac{1}{2}}t^{-1} [nP_n(z) + (2n-1)P_{n-1}(z) + (2n-3)P_{n-2}(z) + ...] \\ &amp; \left( \text{using } \quad (2n+1)^{-\frac{1}{2}} g_n(t, x) = P_n(\frac{2x}{t}-1) = P_n(z)\right) \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} g_n(t, x) + (2n-1)^{\frac{1}{2}} g_{n-1}(t, x) + (2n-3)^{\frac{1}{2}}g_{n-2}(t, x) + ...] \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} g_n^{(t)} + (2n-1)^{\frac{1}{2}} g_{n-1}^{(t)} + (2n-3)^{\frac{1}{2}}g_{n-2}^{(t)} + ...] \\ \end{aligned}\] <ul> <li><strong>HiPPO-LegS Coefficient Dynamics</strong></li> </ul> \[\begin{aligned} \frac{d}{dt} c_n(t) &amp;= \frac{d}{dt} ( \int f {p_n}^{(t)} \omega^{(t)} dx) \\ &amp;= \int f (\textcolor{red}{\frac{\partial}{\partial t} g_n^{(t)}}) \omega^{(t)} dx + \int f g_n^{(t)} \textcolor{green}{\frac{\partial}{\partial t} \omega^{(t)}} dx \\ &amp;= \int f \left( \textcolor{red}{ -t^{-1}(2n+1)^{\frac{1}{2}} \left[ n (2n+1)^{-\frac{1}{2}} g_n^{(t)} + (2n-1)^{\frac{1}{2}} g_{n-1}^{(t)} + ...\right] } \right) \omega^{(t)} dx + \left( \int f g_n^{(t)} \textcolor{green}{t^{-1} (-\omega^{(t)} + \delta_t)} dx \right) \\ &amp;= \left( -t^{-1}(2n+1)^{\frac{1}{2}} \left[n (2n+1)^{-\frac{1}{2}} \textcolor{red}{\int f g_n^{(t)} \omega^{(t)} dx} + (2n-1)^{\frac{1}{2}} \textcolor{red}{\int f g_{n-1}^{(t)} \omega^{(t)} dx} + ...\right] \right) + \left( \int f g_n^{(t)} t^{-1} (-\omega^{(t)} + \delta_t) dx \right) \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} \textcolor{red}{c_n(t)} + (2n-1)^{\frac{1}{2}} \textcolor{red}{c_{n-1}(t)} + (2n-3)^{\frac{1}{2}} \textcolor{red}{c_{n-2}(t)} + ...] + \left( \int f g_n^{(t)} t^{-1} (-\omega^{(t)} + \delta_t) dx \right) \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}} c_{n-2}(t) + ...] + \left( - t^{-1} \textcolor{green}{ \int f g_n^{(t)} \omega^{(t)}dx} + t^{-1} \textcolor{green}{ \int f g_n^{(t)} \delta_t dx} \right) \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}} c_{n-2}(t) + ...] -t^{-1} \textcolor{green}{c_n(t)} + t^{-1} \textcolor{green}{f(t) g_{n}^{(t)}(t)} \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}} c_{n-2}(t) + ...] -t^{-1} c_n(t) + t^{-1} \textcolor{green}{f(t) (2n+1)^{\frac{1}{2}}P_n(1)} \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [n (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}} c_{n-2}(t) + ...] \textcolor{red}{-t^{-1}c_n(t)} + t^{-1} f(t) (2n+1)^{\frac{1}{2}} \\ &amp;= -t^{-1}(2n+1)^{\frac{1}{2}} [\textcolor{red}{(n+1)} (2n+1)^{-\frac{1}{2}} c_n(t) + (2n-1)^{\frac{1}{2}} c_{n-1}(t) + (2n-3)^{\frac{1}{2}} c_{n-2}(t) + ...] + t^{-1} f(t) (2n+1)^{\frac{1}{2}} \\ \end{aligned}\] <p>For example:</p> \[\begin{aligned} \frac{d}{dt} c_0(t) &amp;= -\frac{1}{t} [(n+1) c_0(t)] = -\frac{1}{t} [1 c_0(t)] \\ \frac{d}{dt} c_1(t) &amp;= -\frac{1}{t} [(n+1) c_1(t) + (2n+1)^{\frac{1}{2}}(2n-1)^{\frac{1}{2}}c_0(t)] = -\frac{1}{t} [2 c_1(t) + 3^{\frac{1}{2}}1^{\frac{1}{2}}c_0] \\ \frac{d}{dt} c_2(t) &amp;= -\frac{1}{t} [(n+1) c_2(t) + (2n+1)^{\frac{1}{2}}(2n-1)^{\frac{1}{2}}c_1(t)+ (2n+1)^{\frac{1}{2}}(2n-3)^{\frac{1}{2}}c_0(t)] \\ &amp; \quad = -\frac{1}{t} [3 c_2(t) + 5^{\frac{1}{2}}3^{\frac{1}{2}}c_1+ 5^{\frac{1}{2}}1^{\frac{1}{2}}c_0] \\ &amp; ... \\ \frac{d}{dt} c_n(t) &amp;= -\frac{1}{t} [(n+1) c_n(t) + \sum_{k=n-1}^{0}(2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}}c_k(t)] \\ \end{aligned}\] <p>Writing the result into the matrix form:</p> \[\begin{aligned} \frac{d}{dt} c_n(t) = -\frac{1}{t} A c(t) + \frac{1}{t} B f(t) \end{aligned}\] <p>where</p> \[\begin{aligned} A_{nk} &amp;= \begin{cases} (2n+1)^{\frac{1}{2}}(2k+1)^{\frac{1}{2}},&amp; \text{if } n &gt; k \\ n+1,&amp; \text{if } n = k \\ 0,&amp; \text{if } n &lt; k \\ \end{cases} \\ B &amp;= (2n+1)^{\frac{1}{2}} \end{aligned}\] <p>We can also write the result into the form: \(\begin{aligned} \frac{d}{dt} c_n(t) = -\frac{1}{t} D[ MD^{-1}c(t) + \mathbb{1} f(t)] \end{aligned}\)</p> <p>where</p> \[\begin{aligned} D &amp;= \text{diag}[(2n+1)^{\frac{1}{2}}]_0^{n-1} \\ M_{nk} &amp;= \begin{cases} 2k+1,&amp; \text{if } n &gt; k \\ k+1,&amp; \text{if } n = k \\ 0,&amp; \text{if } n &lt; k \\ \end{cases} \end{aligned}\] <ul> <li><strong>HiPPO-LegS Reconstruction</strong></li> </ul> <p>Plug \(\chi, \zeta, \lambda_n\) into <a href="#eq:4">[eq:4]</a>, we have</p> \[\begin{aligned} f(x)_{x \leq t} \approx {g}^{(t)} &amp;= \sum_{n=0}^{N-1} \lambda_n^{-1} \zeta^{\frac{1}{2}} c_n(t) {p_n}^{(t)} \chi^{(t)} \\ &amp;= \sum_{n=0}^{N-1} c_n(t) {p_n}^{(t)} = \sum_{n=0}^{N-1} c_n(t) {g_n}(t, x)\\ &amp;= \sum_{n=0}^{N-1} c_n(t) (2n+1)^{\frac{1}{2}}P_n(\frac{2x}{t}-1) \end{aligned}\] <p><br></p> <h1 id="references">References</h1> <p><a id="1">[1]</a> A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré, “Hippo: Recurrent memory with optimal polynomial projections,” Advances in Neural Information Processing Systems, vol. 33, pp. 1474– 1487, 2020.</p> <p><a id="2">[2]</a> A. Gu, K. Goel, A. Gupta, and C. Ré, “On the parameterization and initialization of diagonal state space models,” Advances in Neural Information Processing Systems, vol. 35, pp. 35971–35983, 2022.</p> <p><a id="3">[3]</a> A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” arXiv preprint arXiv:2312.00752, 2023.</p> <p><a id="4">[4]</a> <a href="https://en.wikipedia.org/wiki/Legendre_polynomials" rel="external nofollow noopener" target="_blank">https://en.wikipedia.org/wiki/Legendre_polynomials</a></p> <p><a id="5">[5]</a> <a href="https://mathworld.wolfram.com/LegendrePolynomial.html" rel="external nofollow noopener" target="_blank">https://mathworld.wolfram.com/LegendrePolynomial.html</a></p> </article><div id="disqus_thread" style="max-width: 1000px; margin: 0 auto"></div> <script type="text/javascript">var disqus_shortname="https-hychiang-info",disqus_identifier="/blog/2024/hippo_matrices",disqus_title="HiPPO Matrices";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=190&amp;t=tt&amp;d=63L-AOUUXV7dNP9sE0J35-A9m9tVx4XT7qV2CT9AAC8&amp;co=2d78ad&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=ffffff"></script> <div class="container"> © Copyright 2026 Hung-Yueh Chiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 11, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"},chtml:{scale:.9},svg:{scale:.9}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>