<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization | Hung-Yueh Chiang</title> <meta name="author" content="Hung-Yueh Chiang"> <meta name="description" content="Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/ut_shield.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="/blog/2025/PEQA-summary/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <img width="60px" src="/assets/img/ut_longhorns.webp" class="navbar-brand logo" alt="blank logo"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Hung-Yueh </span>Chiang</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</h1> <p class="post-meta">February 23, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fas fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/paper"> <i class="fas fa-hashtag fa-sm"></i> paper</a>   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/quantization"> <i class="fas fa-hashtag fa-sm"></i> quantization</a>   <a href="/blog/tag/finetuning"> <i class="fas fa-hashtag fa-sm"></i> finetuning</a>   </p> </header> <article class="post-content"> <style>li{font-size:1.1rem}</style> <h1 id="memory-efficient-fine-tuning-of-compressed-large-language-models-via-sub-4-bit-integer-quantization"><a href="https://arxiv.org/abs/2305.14152" rel="external nofollow noopener" target="_blank">Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization</a></h1> <blockquote> <p>[TL;DR]<br> PEQA is a novel fine-tuning approach that integrates Parameter-Efficient Fine-Tuning (PEFT) with <em>weight-only</em> quantized LLMs by updating only the quantization scales, preserving low-bit integer weight matrices. This results in huge memory savings, seamless task adaptation, and inference acceleration.</p> </blockquote> <h2 id="highlights">Highlights</h2> <ul> <li>Integrates PEFT with quantized LLMs, updating only the <strong>quantization scales</strong> while keeping integer matrices frozen.</li> <li> <strong>Reduces memory</strong> consumption during <strong>fine-tuning</strong> and <strong>deployment</strong>, making LLM adaptation feasible even for resource-constrained settings.</li> <li>Maintains quantization benefits post fine-tuning, ensuring <strong>accelerated inference</strong>.</li> <li>Demonstrates resilience in performance recovery, even for <strong>sub-4-bit quantized models</strong>, on large-scale instruction datasets.</li> <li> <strong>Scales up to 65B parameter models</strong> while achieving performance close to full-precision LoRA fine-tuning.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/results_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/results_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/results_overview-1400.webp"></source> <img src="/assets/img/posts/peqa/results_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="PEQA Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <h2 id="summary">Summary</h2> <ul> <li> <strong>Problem Statement</strong>: LLM fine-tuning is memory-intensive, even with PEFT such as LoRA, as full-precision weights remain a bottleneck. Quantization can reduce memory but is typically applied post-training, which limits the adaptability.</li> <li> <strong>Solution</strong>: PEQA bridges this gap by fine-tuning only the <em>quantization scales</em> of a pre-quantized LLM while keeping the integer weights frozen. This enables task-specific adaptation with minimal overhead.</li> <li> <strong>PEQA Framework</strong>: <ul> <li> <strong>Step 1 Decomposition</strong>: Pre-trained model weights are quantized into sub-4-bit integers with associated scaling factors.</li> <li> <strong>Step 2 Fine-tuning</strong>: Only the quantization scales are updated while maintaining the frozen integer matrix, significantly reducing learnable parameters.</li> </ul> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/framework_overview-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/framework_overview-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/framework_overview-1400.webp"></source> <img src="/assets/img/posts/peqa/framework_overview.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="PEQA Overview" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <h2 id="key-advantages">Key Advantages</h2> <h4 id="memory-efficiency">Memory Efficiency</h4> <ul> <li> <strong>Fine-tunes only the quantization scales</strong>, significantly reducing memory overhead.</li> <li> <strong>Optimized for low-bit integer quantization (≤ 4-bit)</strong> while maintaining high accuracy.</li> </ul> <h4 id="seamless-task-switching">Seamless Task Switching</h4> <ul> <li>PEQA enables <strong>quick and efficient adaptation</strong> across different tasks by <em>swapping</em> quantization scales instead of retraining entire models.</li> </ul> <h4 id="faster-inference">Faster Inference</h4> <ul> <li>The <strong>frozen integer matrix remains intact</strong>, ensuring post-fine-tuning speedup using quantized inference kernels.<br> <br> </li> </ul> <h2 id="experiments">Experiments</h2> <h4 id="memory-and-general-comparison">Memory and General Comparison</h4> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/framework_comparison-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/framework_comparison-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/framework_comparison-1400.webp"></source> <img src="/assets/img/posts/peqa/framework_comparison.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <h4 id="peqa-vs-qat-vs-peftptq">PEQA vs. QAT vs. PEFT+PTQ</h4> <ul> <li>PEQA achieves performance close to QAT, significantly outperforming LoRA + PTQ at <strong>3-bit and 4-bit</strong> precision.</li> <li>Lower perplexity indicates effective fine-tuning of quantized models without sacrificing accuracy.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/peqa_vs_baselines_ppl-1400.webp"></source> <img src="/assets/img/posts/peqa/peqa_vs_baselines_ppl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <h4 id="instruction-tuning-with-alpaca-dataset">Instruction-Tuning with Alpaca Dataset</h4> <ul> <li>Evaluated on <strong>common-sense reasoning and in-context learning tasks</strong> (ARC, PIQA, HellaSwag).</li> <li> <strong>Performance comparable to LoRA</strong>, with additional memory savings and inference acceleration.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/peqa/peqa_instruction_tuning-1400.webp"></source> <img src="/assets/img/posts/peqa/peqa_instruction_tuning.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="Instruction-Tuning Results" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><br></p> <h2 id="notations">Notations</h2> <h4 id="quantized-weights-and-fine-tuning">Quantized Weights and Fine-Tuning</h4> <ul> <li>Weight-only asymmetric quantization:<br> Given a fully-connected layer \(\mathbf{W}_0 \in \mathbb{R}^{n \times m}\), a given bit-width \(b\), per-channel scales and zero-points \(\mathbf{s}_0, \mathbf{z}_0 \in \mathbb{R}^{n \times 1}\), asymmetric quantized pre-trained weights \(\widehat{\mathbf{W}}_0\) can be written as</li> </ul> \[\widehat{\mathbf{W}}_0 = \mathbf{s}_0 \cdot \overline{\mathbf{W}}_0 = \mathbf{s}_0 \cdot \left( \text{clamp} \left( \left\lfloor \frac{\mathbf{W}_0}{\mathbf{s}_0} \right\rfloor + \mathbf{z}_0, 0, 2^b - 1 \right) - \mathbf{z}_0 \right),\] <ul> <li>PEQA fine-tuning modifies only the quantization scale by:<br> \(\widehat{\mathbf{W}} = (\mathbf{s}_0 + \Delta s) \cdot \overline{\mathbf{W}}_0 = (\mathbf{s}_0 + \Delta s) \cdot \left( \text{clamp} \left( \left\lfloor \frac{\mathbf{W}_0}{\mathbf{s}_0} \right\rfloor + \mathbf{z}_0, 0, 2^b - 1 \right) - \mathbf{z}_0 \right)\) where \(\overline{\mathbf{W}}_0\) is frozen, and \(\Delta \mathbf{s} \in \mathbb{R}^{n \times 1}\) represents the gradient update of \(\mathbf{s}_0\) obtained by adaptation to a downstream task. <br> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>PEQA presents a <strong>memory-efficient fine-tuning</strong> approach for quantized LLMs (weight-only quantization). By updating only the quantization scales while keeping integer matrices fixed, PEQA achieves:</p> <ul> <li><strong>Comparable accuracy to full-precision PEFT methods</strong></li> <li><strong>Significant memory savings (up to 4× reduction)</strong></li> <li><strong>Seamless adaptation to new tasks</strong></li> <li><strong>Faster inference without additional post-processing</strong></li> </ul> <p>PEQA enables <strong>scalable and efficient model adaptation</strong> for large-scale language models, ensuring practical deployment on memory-constrained devices.</p> </article><div id="disqus_thread" style="max-width: 1000px; margin: 0 auto"></div> <script type="text/javascript">var disqus_shortname="https-hychiang-info",disqus_identifier="/blog/2025/PEQA-summary",disqus_title="Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&amp;w=190&amp;t=tt&amp;d=63L-AOUUXV7dNP9sE0J35-A9m9tVx4XT7qV2CT9AAC8&amp;co=2d78ad&amp;cmo=3acc3a&amp;cmn=ff5353&amp;ct=ffffff"></script> <div class="container"> © Copyright 2025 Hung-Yueh Chiang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: August 31, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"},chtml:{scale:.9},svg:{scale:.9}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>